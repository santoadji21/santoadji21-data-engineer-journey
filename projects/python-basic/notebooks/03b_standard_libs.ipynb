{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03b — Python Standard Libraries for Data Engineering\n",
    "\n",
    "Python ships with powerful **\"batteries-included\"** modules. Mastering them means fewer  \n",
    "external dependencies and more reliable, portable code.\n",
    "\n",
    "### What you'll learn\n",
    "| # | Module | Why it matters for DE |\n",
    "|---|--------|-----------------------|\n",
    "| 1 | **datetime** | Parse, format, and compute dates/times/timestamps/timezones |\n",
    "| 2 | **os & sys** | Read environment variables, interact with the OS and runtime |\n",
    "| 3 | **logging** | Production-grade logging with levels, formatting, and file output |\n",
    "| 4 | **collections** | Specialized containers: `Counter`, `defaultdict`, `namedtuple` |\n",
    "| 5 | **glob** | Pattern-based file discovery across directories |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. datetime — Dates, Times, Timestamps & Timezones\n",
    "\n",
    "The `datetime` module is the foundation for all time-related work.  \n",
    "In data engineering, you'll constantly parse date strings from CSVs/APIs,  \n",
    "compute durations, and convert between timezones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today       : 2026-02-13\n",
      "Specific    : 2025-07-15\n",
      "Year        : 2025\n",
      "Day of week : 1\n",
      "ISO weekday : 2\n"
     ]
    }
   ],
   "source": [
    "from datetime import date, datetime, time, timedelta, timezone\n",
    "\n",
    "# ============================================================\n",
    "# 1a. Core Types\n",
    "# ============================================================\n",
    "\n",
    "# date(year, month, day) — a calendar date with NO time component\n",
    "today = date.today()          # today() returns the current local date\n",
    "specific = date(2025, 7, 15)  # construct a specific date\n",
    "print(f\"Today       : {today}\")\n",
    "print(f\"Specific    : {specific}\")\n",
    "print(f\"Year        : {specific.year}\")     # .year, .month, .day are integer attributes\n",
    "print(f\"Day of week : {specific.weekday()}\")  # weekday() → 0=Monday ... 6=Sunday\n",
    "print(f\"ISO weekday : {specific.isoweekday()}\")  # isoweekday() → 1=Monday ... 7=Sunday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datetime(year, month, day, hour, minute, second, microsecond)\n",
    "# Combines date + time into a single object.\n",
    "now = datetime.now()            # now() returns current local date+time\n",
    "utc_now = datetime.now(timezone.utc)  # now(tz) returns current time in a specific timezone\n",
    "\n",
    "print(f\"Local now : {now}\")\n",
    "print(f\"UTC now   : {utc_now}\")\n",
    "\n",
    "# Access individual components\n",
    "print(f\"Hour      : {now.hour}\")\n",
    "print(f\"Minute    : {now.minute}\")\n",
    "print(f\"Date part : {now.date()}\")   # .date() extracts just the date\n",
    "print(f\"Time part : {now.time()}\")   # .time() extracts just the time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time(hour, minute, second, microsecond) — time of day with NO date\n",
    "checkout_time = time(11, 0, 0)  # 11:00:00 AM\n",
    "print(f\"Checkout time: {checkout_time}\")\n",
    "print(f\"Hour         : {checkout_time.hour}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1b. Parsing Strings → datetime (strptime)\n",
    "# ============================================================\n",
    "# strptime(string, format) parses a date STRING into a datetime object.\n",
    "# \"strptime\" = \"string parse time\"\n",
    "#\n",
    "# Common format codes:\n",
    "#   %Y = 4-digit year (2025)      %y = 2-digit year (25)\n",
    "#   %m = month (01-12)            %B = full month name (July)\n",
    "#   %d = day (01-31)              %b = abbreviated month (Jul)\n",
    "#   %H = hour 24h (00-23)         %I = hour 12h (01-12)\n",
    "#   %M = minute (00-59)           %S = second (00-59)\n",
    "#   %p = AM/PM\n",
    "\n",
    "# ISO format (most common in databases/APIs)\n",
    "dt1 = datetime.strptime(\"2025-07-15 14:30:00\", \"%Y-%m-%d %H:%M:%S\")\n",
    "print(f\"Parsed ISO     : {dt1}\")\n",
    "\n",
    "# US-style format\n",
    "dt2 = datetime.strptime(\"07/15/2025\", \"%m/%d/%Y\")\n",
    "print(f\"Parsed US date : {dt2}\")\n",
    "\n",
    "# From the hotel dataset: arrival date looks like \"July\" + \"2015\" + \"1\"\n",
    "# We can reconstruct and parse it:\n",
    "date_str = \"July 1, 2015\"\n",
    "dt3 = datetime.strptime(date_str, \"%B %d, %Y\")\n",
    "print(f\"Parsed hotel   : {dt3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1c. Formatting datetime → String (strftime)\n",
    "# ============================================================\n",
    "# strftime(format) formats a datetime INTO a string.\n",
    "# \"strftime\" = \"string format time\"\n",
    "\n",
    "now = datetime.now()\n",
    "\n",
    "print(f\"ISO format     : {now.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Date only      : {now.strftime('%Y-%m-%d')}\")\n",
    "print(f\"Human readable : {now.strftime('%B %d, %Y at %I:%M %p')}\")\n",
    "print(f\"Compact (logs) : {now.strftime('%Y%m%d_%H%M%S')}\")\n",
    "\n",
    "# isoformat() is a shorthand for the standard ISO 8601 format\n",
    "print(f\"isoformat()    : {now.isoformat()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1d. timedelta — Date/Time Arithmetic\n",
    "# ============================================================\n",
    "# timedelta represents a DURATION (difference between two dates/times).\n",
    "# You can add/subtract timedeltas to/from dates and datetimes.\n",
    "\n",
    "from datetime import timedelta\n",
    "\n",
    "today = date.today()\n",
    "\n",
    "# Adding durations\n",
    "tomorrow = today + timedelta(days=1)        # timedelta(days=N) creates a N-day duration\n",
    "next_week = today + timedelta(weeks=1)      # weeks is a shorthand for days=7\n",
    "two_hours_later = datetime.now() + timedelta(hours=2, minutes=30)\n",
    "\n",
    "print(f\"Today          : {today}\")\n",
    "print(f\"Tomorrow       : {tomorrow}\")\n",
    "print(f\"Next week      : {next_week}\")\n",
    "print(f\"+2h30m         : {two_hours_later.strftime('%H:%M:%S')}\")\n",
    "\n",
    "# Subtracting dates gives a timedelta\n",
    "checkin = date(2025, 7, 10)\n",
    "checkout = date(2025, 7, 15)\n",
    "stay = checkout - checkin\n",
    "print(f\"\\nCheck-in       : {checkin}\")\n",
    "print(f\"Check-out      : {checkout}\")\n",
    "print(f\"Stay duration  : {stay.days} days\")   # .days extracts the day count as int\n",
    "print(f\"Total seconds  : {stay.total_seconds()}\")  # total_seconds() converts to float seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1e. Timezones\n",
    "# ============================================================\n",
    "# In production pipelines, ALWAYS store timestamps in UTC.\n",
    "# Convert to local time only for display.\n",
    "#\n",
    "# timezone.utc          → the UTC timezone (built-in, no extra library)\n",
    "# timezone(timedelta())  → create a fixed-offset timezone\n",
    "\n",
    "from datetime import timezone, timedelta\n",
    "\n",
    "# Create timezone-AWARE datetimes (\"aware\" = knows its timezone)\n",
    "utc_now = datetime.now(timezone.utc)\n",
    "print(f\"UTC now       : {utc_now}\")\n",
    "print(f\"UTC isoformat : {utc_now.isoformat()}\")\n",
    "\n",
    "# Create a custom timezone: UTC+7 (Jakarta / WIB)\n",
    "wib = timezone(timedelta(hours=7))   # fixed offset from UTC\n",
    "jakarta_now = utc_now.astimezone(wib) # astimezone() converts to another timezone\n",
    "print(f\"Jakarta (WIB) : {jakarta_now}\")\n",
    "\n",
    "# UTC+9 (Tokyo / JST)\n",
    "jst = timezone(timedelta(hours=9))\n",
    "tokyo_now = utc_now.astimezone(jst)\n",
    "print(f\"Tokyo (JST)   : {tokyo_now}\")\n",
    "\n",
    "# Naive vs Aware:\n",
    "# \"Naive\" datetime has NO timezone info → dangerous in production!\n",
    "naive = datetime.now()              # naive — no tz info\n",
    "aware = datetime.now(timezone.utc)  # aware — knows it's UTC\n",
    "print(f\"\\nNaive tzinfo  : {naive.tzinfo}\")   # None\n",
    "print(f\"Aware tzinfo  : {aware.tzinfo}\")     # UTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unix timestamp : 1771021966.457106\n",
      "As integer     : 1771021966\n",
      "Restored       : 2026-02-13 22:32:46.457106+00:00\n",
      "Match          : True\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 1f. Timestamps (Unix Epoch)\n",
    "# ============================================================\n",
    "# A Unix timestamp is the number of seconds since Jan 1, 1970 00:00 UTC.\n",
    "# It's the universal exchange format for time data between systems.\n",
    "\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "now_utc = datetime.now(timezone.utc)\n",
    "\n",
    "# datetime → timestamp (float seconds since epoch)\n",
    "ts = now_utc.timestamp()  # timestamp() returns a float\n",
    "print(f\"Unix timestamp : {ts}\")\n",
    "print(f\"As integer     : {int(ts)}\")\n",
    "\n",
    "# timestamp → datetime\n",
    "# fromtimestamp(ts, tz) converts back; ALWAYS pass tz=timezone.utc\n",
    "restored = datetime.fromtimestamp(ts, tz=timezone.utc)\n",
    "print(f\"Restored       : {restored}\")\n",
    "print(f\"Match          : {restored == now_utc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Booking #1: booked 2014-07-24 → arrives 2015-07-01  (lead time: 342 days)\n",
      "  Booking #2: booked 2013-06-24 → arrives 2015-07-01  (lead time: 737 days)\n",
      "  Booking #3: booked 2015-06-24 → arrives 2015-07-01  (lead time: 7 days)\n",
      "  Booking #4: booked 2015-06-18 → arrives 2015-07-01  (lead time: 13 days)\n",
      "  Booking #5: booked 2015-06-17 → arrives 2015-07-01  (lead time: 14 days)\n",
      "  Booking #6: booked 2015-06-17 → arrives 2015-07-01  (lead time: 14 days)\n",
      "  Booking #7: booked 2015-07-01 → arrives 2015-07-01  (lead time: 0 days)\n",
      "  Booking #8: booked 2015-06-22 → arrives 2015-07-01  (lead time: 9 days)\n",
      "  Booking #9: booked 2015-04-07 → arrives 2015-07-01  (lead time: 85 days)\n",
      "  Booking #10: booked 2015-04-17 → arrives 2015-07-01  (lead time: 75 days)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 1g. Practical — Parse hotel booking dates\n",
    "# ============================================================\n",
    "# The hotel dataset has: arrival_date_year, arrival_date_month,\n",
    "# arrival_date_day_of_month as SEPARATE columns.\n",
    "# Let's combine them into proper date objects.\n",
    "\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def parse_hotel_date(row: dict) -> date:\n",
    "    \"\"\"\n",
    "    Combine year + month_name + day columns into a date object.\n",
    "    Example: 2015, 'July', 1 → date(2015, 7, 1)\n",
    "    \"\"\"\n",
    "    date_str = f\"{row['arrival_date_month']} {row['arrival_date_day_of_month']}, {row['arrival_date_year']}\"\n",
    "    return datetime.strptime(date_str, \"%B %d, %Y\").date()  # .date() drops the time part\n",
    "\n",
    "\n",
    "# Parse first 5 bookings\n",
    "hotel_csv = Path(\"data/hotel_booking.csv\")\n",
    "\n",
    "with hotel_csv.open(\"r\", newline=\"\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for i, row in enumerate(reader):\n",
    "        if i >= 10:\n",
    "            break\n",
    "        arrival = parse_hotel_date(row)\n",
    "        lead_days = int(row[\"lead_time\"])  # days between booking and arrival\n",
    "        booking_date = arrival - timedelta(days=lead_days)\n",
    "        print(\n",
    "            f\"  Booking #{i+1}: booked {booking_date} → arrives {arrival}\"\n",
    "            f\"  (lead time: {lead_days} days)\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. os & sys — Operating System & Runtime Interaction\n",
    "\n",
    "- `os` — environment variables, process info, filesystem operations  \n",
    "- `sys` — Python runtime info, interpreter settings, command-line args  \n",
    "\n",
    "Critical for making pipelines **configurable** (dev vs prod) via env vars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total env vars : 18\n",
      "HOME         : /root\n",
      "USER         : unknown\n",
      "DATABASE_URL : sqlite:///local.db (fallback — not set)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# ============================================================\n",
    "# 2a. Environment Variables\n",
    "# ============================================================\n",
    "# In production, pipelines read config from environment variables\n",
    "# (database URLs, API keys, feature flags) — NEVER hardcode secrets.\n",
    "\n",
    "# os.environ is a dict-like object of ALL environment variables\n",
    "print(f\"Total env vars : {len(os.environ)}\")\n",
    "\n",
    "# os.getenv(key, default) safely reads an env var.\n",
    "# Returns `default` if the key doesn't exist (instead of raising KeyError).\n",
    "home = os.getenv(\"HOME\", \"/unknown\")        # user's home directory\n",
    "user = os.getenv(\"USER\", \"unknown\")          # current username\n",
    "db_url = os.getenv(\"DATABASE_URL\", \"sqlite:///local.db\")  # fallback to local DB\n",
    "\n",
    "print(f\"HOME         : {home}\")\n",
    "print(f\"USER         : {user}\")\n",
    "print(f\"DATABASE_URL : {db_url} (fallback — not set)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting environment variables at runtime\n",
    "# os.environ[key] = value sets a variable for THIS process and its children.\n",
    "os.environ[\"PIPELINE_ENV\"] = \"development\"\n",
    "os.environ[\"BATCH_SIZE\"] = \"5000\"  # values MUST be strings\n",
    "\n",
    "# Read them back\n",
    "env = os.getenv(\"PIPELINE_ENV\")\n",
    "batch = int(os.getenv(\"BATCH_SIZE\", \"1000\"))  # cast to int after reading\n",
    "\n",
    "print(f\"PIPELINE_ENV : {env}\")\n",
    "print(f\"BATCH_SIZE   : {batch} (type: {type(batch).__name__})\")\n",
    "\n",
    "# Clean up — remove a variable\n",
    "# os.environ.pop(key, default) removes the key, returns its value\n",
    "os.environ.pop(\"PIPELINE_ENV\", None)\n",
    "print(f\"After pop    : {os.getenv('PIPELINE_ENV', 'NOT SET')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 2b. Process & Filesystem Info (os)\n",
    "# ============================================================\n",
    "\n",
    "# os.getcwd() returns the Current Working Directory as a string\n",
    "print(f\"CWD          : {os.getcwd()}\")\n",
    "\n",
    "# os.getpid() returns the Process ID of the current Python process\n",
    "print(f\"Process ID   : {os.getpid()}\")\n",
    "\n",
    "# os.cpu_count() returns the number of CPU cores (useful for parallelism)\n",
    "print(f\"CPU cores    : {os.cpu_count()}\")\n",
    "\n",
    "# os.listdir(path) returns a list of filenames in a directory\n",
    "# (Pathlib's iterdir() is generally preferred, but os.listdir is still common)\n",
    "files = os.listdir(\"data\")\n",
    "print(f\"\\nFiles in data/ ({len(files)}):\")\n",
    "for name in sorted(files):\n",
    "    print(f\"  {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# ============================================================\n",
    "# 2c. Python Runtime Info (sys)\n",
    "# ============================================================\n",
    "\n",
    "# sys.version — full Python version string\n",
    "print(f\"Python version  : {sys.version}\")\n",
    "\n",
    "# sys.version_info — structured version (major, minor, micro, ...)\n",
    "v = sys.version_info\n",
    "print(f\"Version tuple   : {v.major}.{v.minor}.{v.micro}\")\n",
    "\n",
    "# sys.platform — OS identifier ('linux', 'darwin' for macOS, 'win32')\n",
    "print(f\"Platform        : {sys.platform}\")\n",
    "\n",
    "# sys.executable — path to the Python interpreter binary\n",
    "print(f\"Executable      : {sys.executable}\")\n",
    "\n",
    "# sys.path — list of directories Python searches for imports\n",
    "# (useful for debugging \"module not found\" errors)\n",
    "print(f\"\\nFirst 3 sys.path entries:\")\n",
    "for p in sys.path[:3]:\n",
    "    print(f\"  {p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 2d. sys.getsizeof — check object memory usage\n",
    "# ============================================================\n",
    "# sys.getsizeof(obj) returns the memory size of a Python object in BYTES.\n",
    "# Helpful for understanding memory consumption in pipelines.\n",
    "\n",
    "small_list = list(range(100))\n",
    "big_list = list(range(1_000_000))\n",
    "sample_dict = {\"booking_id\": \"B-1001\", \"guest\": \"Alya\", \"revenue\": 120.0}\n",
    "sample_str = \"A\" * 10_000\n",
    "\n",
    "print(f\"list(100)        : {sys.getsizeof(small_list):>10,} bytes\")\n",
    "print(f\"list(1_000_000)  : {sys.getsizeof(big_list):>10,} bytes\")\n",
    "print(f\"dict (3 keys)    : {sys.getsizeof(sample_dict):>10,} bytes\")\n",
    "print(f\"str (10K chars)  : {sys.getsizeof(sample_str):>10,} bytes\")\n",
    "\n",
    "# Note: getsizeof only measures the CONTAINER, not nested objects.\n",
    "# For deep measurement, you'd need a recursive function or `pympler`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. logging — Production-Grade Logging\n",
    "\n",
    "**`print()` is for debugging. `logging` is for production.**\n",
    "\n",
    "The `logging` module provides:\n",
    "- **Severity levels** — DEBUG, INFO, WARNING, ERROR, CRITICAL  \n",
    "- **Customizable format** — timestamps, module names, line numbers  \n",
    "- **Multiple outputs** — console, file, remote servers  \n",
    "- **Filtering** — show only WARNING+ in prod, DEBUG+ in dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-13 22:36:06 | DEBUG    | Loading configuration...\n",
      "2026-02-13 22:36:06 | INFO     | Pipeline started successfully\n",
      "2026-02-13 22:36:06 | WARNING  | CSV file has 15 empty rows\n",
      "2026-02-13 22:36:06 | ERROR    | Failed to connect to database\n",
      "2026-02-13 22:36:06 | CRITICAL | Out of disk space — aborting\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "# ============================================================\n",
    "# 3a. Basic Setup with basicConfig\n",
    "# ============================================================\n",
    "# logging.basicConfig() configures the ROOT logger (global default).\n",
    "#   level  : minimum severity to display (everything below is ignored)\n",
    "#   format : template for log messages\n",
    "#   force  : True resets any existing config (needed in notebooks)\n",
    "#\n",
    "# Severity levels (from lowest to highest):\n",
    "#   DEBUG    (10) — detailed diagnostic info (verbose)\n",
    "#   INFO     (20) — confirmation that things work as expected\n",
    "#   WARNING  (30) — something unexpected but not an error (default)\n",
    "#   ERROR    (40) — a serious problem; some functionality failed\n",
    "#   CRITICAL (50) — the program may not be able to continue\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,  # show everything from DEBUG and above\n",
    "    format=\"%(asctime)s | %(levelname)-8s | %(message)s\",\n",
    "    # Format codes:\n",
    "    #   %(asctime)s   — timestamp string\n",
    "    #   %(levelname)s — severity level name (e.g. 'INFO')\n",
    "    #   %(message)s   — the actual log message\n",
    "    #   %(name)s      — logger name (default: 'root')\n",
    "    #   %(filename)s  — source file name\n",
    "    #   %(lineno)d    — line number in source file\n",
    "    #   %-8s          — left-align and pad to 8 chars\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",  # custom timestamp format\n",
    "    force=True,  # reset any prior config (important in Jupyter)\n",
    ")\n",
    "\n",
    "# Each level has its own function\n",
    "logging.debug(\"Loading configuration...\")       # verbose detail\n",
    "logging.info(\"Pipeline started successfully\")    # normal operation\n",
    "logging.warning(\"CSV file has 15 empty rows\")    # unexpected but recoverable\n",
    "logging.error(\"Failed to connect to database\")   # something broke\n",
    "logging.critical(\"Out of disk space — aborting\")  # fatal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-13 22:36:11 | INFO     | Processing hotel_booking.csv\n",
      "2026-02-13 22:36:11 | WARNING  | Found 42 bookings with negative ADR\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 3b. Named Loggers — scoped logging for modules\n",
    "# ============================================================\n",
    "# In real projects, each module gets its OWN named logger.\n",
    "# This lets you control log levels per module.\n",
    "#\n",
    "# logging.getLogger(name) creates or retrieves a named logger.\n",
    "# Convention: use __name__ so the logger matches the module name.\n",
    "\n",
    "logger = logging.getLogger(\"hotel_pipeline\")\n",
    "\n",
    "# You can set a DIFFERENT level for this specific logger\n",
    "logger.setLevel(logging.INFO)  # only INFO and above for this logger\n",
    "\n",
    "logger.debug(\"This won't appear — level is INFO\")  # below INFO → suppressed\n",
    "logger.info(\"Processing hotel_booking.csv\")\n",
    "logger.warning(\"Found 42 bookings with negative ADR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3c. Logging with Variables — f-string vs % formatting\n",
    "# ============================================================\n",
    "# Two ways to include variables in log messages:\n",
    "\n",
    "total_rows = 119_390\n",
    "skipped = 42\n",
    "\n",
    "# Method 1: %-style (traditional, lazy evaluation — only formats if level is shown)\n",
    "logger.info(\"Processed %d rows, skipped %d\", total_rows, skipped)\n",
    "\n",
    "# Method 2: f-string (modern, always evaluates even if log is suppressed)\n",
    "logger.info(f\"Processed {total_rows:,} rows, skipped {skipped}\")\n",
    "\n",
    "# Best practice: Use %-style for hot paths (millions of calls),\n",
    "# f-strings are fine for most DE pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3d. Logging to a File\n",
    "# ============================================================\n",
    "# FileHandler writes log messages to a file on disk.\n",
    "# You can have MULTIPLE handlers: one for console, one for file.\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "log_dir = Path(\"data/output\")\n",
    "log_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create a fresh logger for this demo\n",
    "file_logger = logging.getLogger(\"file_demo\")\n",
    "file_logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Remove any existing handlers (cleanup for re-running in notebook)\n",
    "file_logger.handlers.clear()\n",
    "\n",
    "# FileHandler(filename, mode) — mode 'a' appends, 'w' overwrites\n",
    "fh = logging.FileHandler(log_dir / \"pipeline.log\", mode=\"w\")\n",
    "fh.setLevel(logging.DEBUG)\n",
    "\n",
    "# Formatter controls the output format for this handler\n",
    "formatter = logging.Formatter(\n",
    "    \"%(asctime)s | %(name)s | %(levelname)-8s | %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "fh.setFormatter(formatter)     # attach the format to the handler\n",
    "file_logger.addHandler(fh)     # attach the handler to the logger\n",
    "\n",
    "# Also add a StreamHandler for console output\n",
    "sh = logging.StreamHandler()   # StreamHandler() writes to stderr by default\n",
    "sh.setLevel(logging.INFO)      # console only shows INFO+\n",
    "sh.setFormatter(formatter)\n",
    "file_logger.addHandler(sh)\n",
    "\n",
    "# Now log some messages\n",
    "file_logger.debug(\"Debug: loading config\")   # → file only (below console level)\n",
    "file_logger.info(\"Info: pipeline started\")    # → file + console\n",
    "file_logger.warning(\"Warning: 15 null rows\")  # → file + console\n",
    "\n",
    "# Show the log file contents\n",
    "print(\"\\n--- pipeline.log ---\")\n",
    "print((log_dir / \"pipeline.log\").read_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3e. Logging Exceptions with traceback\n",
    "# ============================================================\n",
    "# logger.exception() logs at ERROR level and INCLUDES the full traceback.\n",
    "# Always use it inside an `except` block.\n",
    "\n",
    "try:\n",
    "    result = int(\"not_a_number\")\n",
    "except ValueError:\n",
    "    # exception() automatically captures the traceback from the current exception\n",
    "    file_logger.exception(\"Failed to parse value\")\n",
    "    # This logs:\n",
    "    #   ERROR | Failed to parse value\n",
    "    #   Traceback (most recent call last): ...\n",
    "    #   ValueError: invalid literal for int() ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. collections — Specialized Data Structures\n",
    "\n",
    "The `collections` module provides high-performance alternatives to plain dicts/lists.  \n",
    "These are the ones you'll use **constantly** in DE pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Room counts: Counter({'Suite': 3, 'Deluxe': 2, 'Standard': 2})\n",
      "Most common: [('Suite', 3), ('Deluxe', 2)]\n",
      "Suite count : 3\n",
      "Penthouse   : 0\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# ============================================================\n",
    "# 4a. Counter — count occurrences of anything\n",
    "# ============================================================\n",
    "# Counter is a dict subclass: {element: count}.\n",
    "# Like SQL: SELECT value, COUNT(*) FROM ... GROUP BY value\n",
    "\n",
    "# Count from a list\n",
    "room_types = [\"Suite\", \"Deluxe\", \"Standard\", \"Suite\", \"Deluxe\", \"Suite\", \"Standard\"]\n",
    "room_counts = Counter(room_types)\n",
    "print(f\"Room counts: {room_counts}\")\n",
    "# Counter({'Suite': 3, 'Deluxe': 2, 'Standard': 2})\n",
    "\n",
    "# most_common(n) returns the n most frequent elements as (element, count) tuples\n",
    "print(f\"Most common: {room_counts.most_common(2)}\")\n",
    "\n",
    "# Access count for a specific element (returns 0 if not found — never KeyError!)\n",
    "print(f\"Suite count : {room_counts['Suite']}\")\n",
    "print(f\"Penthouse   : {room_counts['Penthouse']}\")  # 0, not KeyError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique countries: 178\n",
      "\n",
      "Top 10 countries:\n",
      "    PRT : 48,590 bookings\n",
      "    GBR : 12,129 bookings\n",
      "    FRA : 10,415 bookings\n",
      "    ESP :  8,568 bookings\n",
      "    DEU :  7,287 bookings\n",
      "    ITA :  3,766 bookings\n",
      "    IRL :  3,375 bookings\n",
      "    BEL :  2,342 bookings\n",
      "    BRA :  2,224 bookings\n",
      "    NLD :  2,104 bookings\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 4b. Counter — practical: count countries in hotel CSV\n",
    "# ============================================================\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "hotel_csv = Path(\"data/hotel_booking.csv\")\n",
    "\n",
    "# Stream the CSV and count countries using a generator expression\n",
    "with hotel_csv.open(\"r\", newline=\"\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    # Counter() accepts any iterable — here, a generator of country values\n",
    "    country_counts = Counter(row[\"country\"] for row in reader)\n",
    "\n",
    "print(f\"Unique countries: {len(country_counts)}\")\n",
    "print(f\"\\nTop 10 countries:\")\n",
    "for country, count in country_counts.most_common(10):\n",
    "    print(f\"  {country:>5} : {count:>6,} bookings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total (Jul+Aug) : Counter({'Standard': 150, 'Suite': 95, 'Deluxe': 85})\n",
      "Aug gained over Jul: Counter({'Deluxe': 25})\n",
      "Grand total     : 330\n"
     ]
    }
   ],
   "source": [
    "# Counter arithmetic — combine or subtract counts\n",
    "july_rooms = Counter({\"Suite\": 50, \"Deluxe\": 30, \"Standard\": 80})\n",
    "aug_rooms = Counter({\"Suite\": 45, \"Deluxe\": 55, \"Standard\": 70})\n",
    "\n",
    "# Addition: combine counts\n",
    "total = july_rooms + aug_rooms\n",
    "print(f\"Total (Jul+Aug) : {total}\")\n",
    "\n",
    "# Subtraction: difference (negative counts are dropped)\n",
    "diff = aug_rooms - july_rooms\n",
    "print(f\"Aug gained over Jul: {diff}\")\n",
    "\n",
    "# total() returns the sum of all counts (Python 3.10+)\n",
    "try:\n",
    "    print(f\"Grand total     : {total.total()}\")\n",
    "except AttributeError:\n",
    "    print(f\"Grand total     : {sum(total.values())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# ============================================================\n",
    "# 4c. defaultdict — auto-initialize missing keys\n",
    "# ============================================================\n",
    "# A regular dict raises KeyError on missing keys.\n",
    "# defaultdict(factory) automatically creates a default value\n",
    "# using the factory function when you access a missing key.\n",
    "#\n",
    "# Common factories:\n",
    "#   defaultdict(int)   → missing key = 0    (great for counting)\n",
    "#   defaultdict(list)  → missing key = []   (great for grouping)\n",
    "#   defaultdict(set)   → missing key = set()\n",
    "#   defaultdict(float) → missing key = 0.0\n",
    "\n",
    "# --- Example: Group bookings by hotel type ---\n",
    "# Without defaultdict: you'd need `if key not in d: d[key] = []` every time.\n",
    "\n",
    "bookings_by_hotel = defaultdict(list)  # missing key → empty list\n",
    "\n",
    "with hotel_csv.open(\"r\", newline=\"\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for i, row in enumerate(reader):\n",
    "        if i >= 20:  # just first 20 rows for demo\n",
    "            break\n",
    "        hotel = row[\"hotel\"]\n",
    "        # No need to check if key exists — defaultdict creates [] automatically\n",
    "        bookings_by_hotel[hotel].append(row[\"country\"])\n",
    "\n",
    "for hotel, countries in bookings_by_hotel.items():\n",
    "    print(f\"{hotel}: {len(countries)} bookings from {countries}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- defaultdict(int) for counting ---\n",
    "# Same as Counter, but useful when you need more control over the counting logic.\n",
    "\n",
    "revenue_by_hotel = defaultdict(float)  # missing key → 0.0\n",
    "count_by_hotel = defaultdict(int)      # missing key → 0\n",
    "\n",
    "with hotel_csv.open(\"r\", newline=\"\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        hotel = row[\"hotel\"]\n",
    "        try:\n",
    "            adr = float(row[\"adr\"])\n",
    "        except (ValueError, KeyError):\n",
    "            adr = 0.0\n",
    "        revenue_by_hotel[hotel] += adr\n",
    "        count_by_hotel[hotel] += 1\n",
    "\n",
    "print(\"Hotel Revenue Summary:\")\n",
    "for hotel in revenue_by_hotel:\n",
    "    avg = revenue_by_hotel[hotel] / count_by_hotel[hotel]\n",
    "    print(f\"  {hotel:>15} : {count_by_hotel[hotel]:>6,} bookings, avg ADR ${avg:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guest: Alya, Hotel: Resort Hotel, ADR: $120.0\n",
      "First field: B-1001\n",
      "Unpacked: Rafi at City Hotel\n",
      "As dict: {'booking_id': 'B-1001', 'guest': 'Alya', 'hotel': 'Resort Hotel', 'adr': 120.0, 'country': 'IDN'}\n",
      "Updated ADR: 150.0 (original: 120.0)\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "# ============================================================\n",
    "# 4d. namedtuple — lightweight, immutable data records\n",
    "# ============================================================\n",
    "# namedtuple creates a class with named fields but no methods.\n",
    "# Like a dict but:\n",
    "#   - Immutable (can't change values after creation)\n",
    "#   - Uses less memory than a dict\n",
    "#   - Access by NAME (record.field) and by INDEX (record[0])\n",
    "#\n",
    "# Great for representing structured records in pipelines.\n",
    "\n",
    "# Define a namedtuple type (like defining a class)\n",
    "# namedtuple(typename, field_names)\n",
    "Booking = namedtuple(\"Booking\", [\"booking_id\", \"guest\", \"hotel\", \"adr\", \"country\"])\n",
    "\n",
    "# Create instances\n",
    "b1 = Booking(\"B-1001\", \"Alya\", \"Resort Hotel\", 120.0, \"IDN\")\n",
    "b2 = Booking(\"B-1002\", \"Rafi\", \"City Hotel\", 95.0, \"PRT\")\n",
    "\n",
    "# Access by name (preferred — more readable)\n",
    "print(f\"Guest: {b1.guest}, Hotel: {b1.hotel}, ADR: ${b1.adr}\")\n",
    "\n",
    "# Access by index (like a regular tuple)\n",
    "print(f\"First field: {b1[0]}\")\n",
    "\n",
    "# Unpack like a tuple\n",
    "bid, guest, hotel, adr, country = b2\n",
    "print(f\"Unpacked: {guest} at {hotel}\")\n",
    "\n",
    "# Convert to dict\n",
    "print(f\"As dict: {b1._asdict()}\")\n",
    "\n",
    "# _replace() creates a NEW namedtuple with some fields changed (immutable!)\n",
    "b1_updated = b1._replace(adr=150.0)\n",
    "print(f\"Updated ADR: {b1_updated.adr} (original: {b1.adr})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict, deque\n",
    "\n",
    "# ============================================================\n",
    "# 4e. Other Useful collections\n",
    "# ============================================================\n",
    "\n",
    "# --- OrderedDict ---\n",
    "# Dict that remembers insertion order.\n",
    "# Note: regular dicts preserve order since Python 3.7+,\n",
    "# but OrderedDict has extra features like move_to_end().\n",
    "od = OrderedDict()\n",
    "od[\"step_1\"] = \"extract\"\n",
    "od[\"step_2\"] = \"transform\"\n",
    "od[\"step_3\"] = \"load\"\n",
    "\n",
    "# move_to_end(key, last=True/False) reorders a key\n",
    "od.move_to_end(\"step_1\", last=True)  # move step_1 to the end\n",
    "print(f\"OrderedDict: {list(od.items())}\")\n",
    "\n",
    "# --- deque (double-ended queue) ---\n",
    "# Efficient append/pop from BOTH ends — O(1) vs O(n) for lists.\n",
    "# Great for: sliding windows, recent-N caches, BFS algorithms.\n",
    "\n",
    "# deque(maxlen=N) automatically drops the oldest item when full\n",
    "recent_errors = deque(maxlen=3)  # keep only last 3 errors\n",
    "\n",
    "recent_errors.append(\"Error at row 100\")   # append() adds to the right\n",
    "recent_errors.append(\"Error at row 250\")\n",
    "recent_errors.append(\"Error at row 500\")\n",
    "recent_errors.append(\"Error at row 800\")   # oldest (row 100) is dropped!\n",
    "\n",
    "print(f\"\\nRecent errors (max 3): {list(recent_errors)}\")\n",
    "\n",
    "# appendleft() adds to the left (front)\n",
    "recent_errors.appendleft(\"URGENT: row 1\")\n",
    "print(f\"After appendleft    : {list(recent_errors)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. glob — Pattern-Based File Discovery\n",
    "\n",
    "The `glob` module finds files matching shell-style wildcard patterns.  \n",
    "While `pathlib.glob()` is preferred for new code (see notebook 03),  \n",
    "the standalone `glob` module is still widely used in legacy code and scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV files in data/:\n",
      "  data/bookings.csv\n",
      "  data/hotel_booking.csv\n",
      "\n",
      "JSON/JSONL files (recursive):\n",
      "  data/booking_summary.json\n",
      "  data/bookings.json\n"
     ]
    }
   ],
   "source": [
    "import glob as glob_module  # import with alias to avoid conflict with pathlib.glob\n",
    "\n",
    "# ============================================================\n",
    "# 5a. Basic Patterns\n",
    "# ============================================================\n",
    "# glob.glob(pattern) returns a list of matching file paths as strings.\n",
    "#\n",
    "# Wildcard patterns:\n",
    "#   *     → matches everything (any characters, any length)\n",
    "#   ?     → matches any single character\n",
    "#   [abc] → matches one character from the set\n",
    "#   **    → matches any depth of directories (requires recursive=True)\n",
    "\n",
    "# Find all CSV files in data/\n",
    "csvs = sorted(glob_module.glob(\"data/*.csv\"))\n",
    "print(\"CSV files in data/:\")\n",
    "for f in csvs:\n",
    "    print(f\"  {f}\")\n",
    "\n",
    "# Find all JSON and JSONL files\n",
    "json_files = sorted(glob_module.glob(\"data/**/*.json*\", recursive=True))\n",
    "print(f\"\\nJSON/JSONL files (recursive):\")\n",
    "for f in json_files:\n",
    "    print(f\"  {f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files in data/ (iglob iterator):\n",
      "  data/booking_summary.json\n",
      "  data/bookings.json\n",
      "  data/hotel_booking.csv\n",
      "  data/bookings.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 5b. iglob — Lazy Iterator (memory efficient)\n",
    "# ============================================================\n",
    "# glob.glob() returns a full list (loads all matches into memory).\n",
    "# glob.iglob() returns an ITERATOR — yields one match at a time.\n",
    "# Preferred when you might have thousands of matches.\n",
    "\n",
    "print(\"All files in data/ (iglob iterator):\")\n",
    "for filepath in glob_module.iglob(\"data/**/*\", recursive=True):\n",
    "    print(f\"  {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 5c. Practical — discover and summarize data files\n",
    "# ============================================================\n",
    "import os\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def summarize_data_directory(directory: str) -> None:\n",
    "    \"\"\"\n",
    "    Scan a directory recursively and summarize files by extension.\n",
    "    Combines glob (discovery), pathlib (info), and Counter (aggregation).\n",
    "    \"\"\"\n",
    "    # Use pathlib's rglob for recursive discovery\n",
    "    data_dir = Path(directory)\n",
    "    all_files = list(data_dir.rglob(\"*\"))  # rglob('*') matches all files recursively\n",
    "\n",
    "    # Filter to files only (not directories)\n",
    "    files_only = [f for f in all_files if f.is_file()]\n",
    "\n",
    "    # Count by extension\n",
    "    ext_counts = Counter(f.suffix.lower() for f in files_only)\n",
    "\n",
    "    # Sum sizes by extension\n",
    "    ext_sizes: dict[str, int] = {}\n",
    "    for f in files_only:\n",
    "        ext = f.suffix.lower()\n",
    "        ext_sizes[ext] = ext_sizes.get(ext, 0) + f.stat().st_size\n",
    "\n",
    "    print(f\"Directory: {data_dir.resolve()}\")\n",
    "    print(f\"Total files: {len(files_only)}\\n\")\n",
    "    print(f\"{'Extension':>10} | {'Count':>5} | {'Size':>10}\")\n",
    "    print(f\"{'-'*10:>10} | {'-'*5:>5} | {'-'*10:>10}\")\n",
    "\n",
    "    for ext, count in ext_counts.most_common():\n",
    "        size_kb = ext_sizes[ext] / 1024\n",
    "        unit = \"KB\" if size_kb < 1024 else \"MB\"\n",
    "        size_val = size_kb if size_kb < 1024 else size_kb / 1024\n",
    "        print(f\"{ext:>10} | {count:>5} | {size_val:>7.1f} {unit}\")\n",
    "\n",
    "\n",
    "summarize_data_directory(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 5d. Comparing glob approaches\n",
    "# ============================================================\n",
    "# Summary of the 3 ways to find files by pattern:\n",
    "#\n",
    "# | Approach            | Returns   | Recursive       | Import           |\n",
    "# |---------------------|-----------|-----------------|------------------|\n",
    "# | glob.glob()         | list[str] | recursive=True  | import glob      |\n",
    "# | glob.iglob()        | iterator  | recursive=True  | import glob      |\n",
    "# | Path.glob() / rglob | generator | rglob() = **/*  | from pathlib ... |\n",
    "#\n",
    "# Recommendation: Use pathlib.rglob() for new code — it returns Path objects\n",
    "# with all the useful methods (.stat(), .suffix, .name, etc.).\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# pathlib equivalent of glob.glob('data/**/*.csv', recursive=True)\n",
    "csv_files = list(Path(\"data\").rglob(\"*.csv\"))\n",
    "print(\"CSV files (pathlib):\")\n",
    "for f in csv_files:\n",
    "    size_kb = f.stat().st_size / 1024\n",
    "    print(f\"  {f.name:>25} — {size_kb:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Key Takeaways\n",
    "\n",
    "| Module | Key Functions | DE Use Case |\n",
    "|--------|--------------|-------------|\n",
    "| **datetime** | `strptime`, `strftime`, `timedelta`, `timezone` | Parse dates from CSVs/APIs, compute durations, store in UTC |\n",
    "| **os** | `getenv`, `environ`, `getcwd`, `cpu_count` | Config via env vars, detect runtime environment |\n",
    "| **sys** | `version`, `platform`, `getsizeof`, `path` | Debug imports, check memory, log runtime info |\n",
    "| **logging** | `basicConfig`, `getLogger`, `FileHandler` | Replace `print()` with structured, leveled log output |\n",
    "| **collections** | `Counter`, `defaultdict`, `namedtuple`, `deque` | Count, group, and structure data efficiently |\n",
    "| **glob** | `glob()`, `iglob()`, pathlib `rglob()` | Discover data files by pattern across directories |\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Continue with **`04_pandas_deep_dive.ipynb`** — Numpy, Pandas transformations, and Parquet format."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 — File Handling for Data Engineering\n",
    "\n",
    "Master native file interaction and efficient processing of large datasets **without heavy libraries**.\n",
    "\n",
    "### What you'll learn\n",
    "| # | Topic | Key Takeaway |\n",
    "|---|-------|--------------|\n",
    "| 1 | **Pathlib Mastery** | Object-oriented filesystem paths, globbing, file info |\n",
    "| 2 | **CSV Module** | Precise control over delimiters, quoting, and types |\n",
    "| 3 | **JSON & JSONL** | Structured data and line-delimited streaming format |\n",
    "| 4 | **Context Managers** | Custom resource handling with `with` statements |\n",
    "| 5 | **Generators** | Memory-efficient large file processing with `yield` |\n",
    "| 6 | **Binary & Buffering** | Reading files in raw byte chunks for performance |\n",
    "| 7 | **Mini Pipeline** | Putting it all together on a real dataset |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Pathlib Mastery\n",
    "\n",
    "`pathlib.Path` is the modern, object-oriented way to work with filesystem paths.  \n",
    "It replaces older `os.path` string manipulation with readable, chainable methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative path : data/bookings.csv\n",
      "Absolute path : /app/notebooks/data/bookings.csv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# --- Path Construction ---\n",
    "# Path() creates a path object from a string.\n",
    "# The `/` operator joins path segments (like os.path.join but cleaner).\n",
    "data_dir = Path(\"data\")\n",
    "csv_path = data_dir / \"bookings.csv\"\n",
    "\n",
    "print(\"Relative path :\", csv_path)            # data/bookings.csv\n",
    "print(\"Absolute path :\", csv_path.resolve())   # resolve() returns the full absolute path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name  : hotel_booking.csv\n",
      "stem  : hotel_booking\n",
      "suffix: .csv\n",
      "parent: data/exports\n",
      "parts : ('data', 'exports', 'hotel_booking.csv')\n"
     ]
    }
   ],
   "source": [
    "# --- Path Components ---\n",
    "# Every path has parts you can inspect without string splitting.\n",
    "sample = Path(\"data/exports/hotel_booking.csv\")\n",
    "\n",
    "print(\"name  :\", sample.name)      # 'hotel_booking.csv'  — filename with extension\n",
    "print(\"stem  :\", sample.stem)      # 'hotel_booking'      — filename WITHOUT extension\n",
    "print(\"suffix:\", sample.suffix)    # '.csv'               — file extension (with dot)\n",
    "print(\"parent:\", sample.parent)    # 'data/exports'       — immediate parent directory\n",
    "print(\"parts :\", sample.parts)     # ('data', 'exports', 'hotel_booking.csv') — all segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Creating Directories ---\n",
    "# mkdir() creates a directory.\n",
    "#   parents=True  → create intermediate dirs if needed (like mkdir -p)\n",
    "#   exist_ok=True → don't error if it already exists\n",
    "output_dir = Path(\"data\") / \"output\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Created:\", output_dir.resolve())\n",
    "print(\"Exists :\", output_dir.exists())   # exists() checks if the path exists on disk\n",
    "print(\"Is dir :\", output_dir.is_dir())   # is_dir() checks if it's a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV files in data/:\n",
      "  bookings.csv\n",
      "  hotel_booking.csv\n",
      "\n",
      "JSON files (recursive):\n",
      "  data/booking_summary.json\n",
      "  data/bookings.json\n"
     ]
    }
   ],
   "source": [
    "# --- Globbing (Pattern Matching) ---\n",
    "# glob() finds files matching a pattern inside a directory.\n",
    "# '*' matches any characters; '**' matches any depth of subdirectories.\n",
    "\n",
    "data_dir = Path(\"data\")\n",
    "\n",
    "# Find all CSV files directly in data/\n",
    "csv_files = sorted(data_dir.glob(\"*.csv\"))\n",
    "print(\"CSV files in data/:\")\n",
    "for f in csv_files:\n",
    "    print(f\"  {f.name}\")\n",
    "\n",
    "# Find all JSON files recursively (any depth)\n",
    "# rglob() is shorthand for glob('**/<pattern>')\n",
    "json_files = sorted(data_dir.rglob(\"*.json\"))\n",
    "print(\"\\nJSON files (recursive):\")\n",
    "for f in json_files:\n",
    "    print(f\"  {f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File   : hotel_booking.csv\n",
      "Size   : 23.95 MB\n",
      "Is file: True\n",
      "\n",
      "Converted path: data/hotel_booking.parquet\n",
      "Renamed path  : data/bookings_clean.csv\n"
     ]
    }
   ],
   "source": [
    "# --- File Info & Manipulation ---\n",
    "hotel_csv = Path(\"data/hotel_booking.csv\")\n",
    "\n",
    "if hotel_csv.exists():\n",
    "    # stat() returns file metadata (size, timestamps, permissions)\n",
    "    info = hotel_csv.stat()\n",
    "    size_mb = info.st_size / (1024 * 1024)   # st_size is in bytes\n",
    "    print(f\"File   : {hotel_csv.name}\")\n",
    "    print(f\"Size   : {size_mb:.2f} MB\")\n",
    "    print(f\"Is file: {hotel_csv.is_file()}\")  # is_file() checks if it's a regular file\n",
    "\n",
    "# with_suffix() returns a NEW path with a different extension (original unchanged)\n",
    "parquet_path = hotel_csv.with_suffix(\".parquet\")\n",
    "print(f\"\\nConverted path: {parquet_path}\")\n",
    "\n",
    "# with_name() returns a NEW path with a different filename\n",
    "renamed_path = hotel_csv.with_name(\"bookings_clean.csv\")\n",
    "print(f\"Renamed path  : {renamed_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. CSV Module — Deep Dive\n",
    "\n",
    "The built-in `csv` module gives precise control over reading/writing CSV files.  \n",
    "Unlike Pandas, it streams row-by-row — great for **memory efficiency** on huge files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 5 rows → data/bookings.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "# ============================================================\n",
    "# 2a. Writing CSV with DictWriter\n",
    "# ============================================================\n",
    "# DictWriter maps dict keys → CSV columns automatically.\n",
    "#   fieldnames : list of column names (controls column order)\n",
    "#   writeheader(): writes the header row\n",
    "#   writerows() : writes all data rows at once\n",
    "\n",
    "csv_path = Path(\"data\") / \"bookings.csv\"\n",
    "\n",
    "rows = [\n",
    "    {\"booking_id\": \"B-1001\", \"guest_name\": \"Alya\",  \"revenue\": 120.0, \"room_type\": \"Suite\"},\n",
    "    {\"booking_id\": \"B-1002\", \"guest_name\": \"Rafi\",  \"revenue\": 180.0, \"room_type\": \"Deluxe\"},\n",
    "    {\"booking_id\": \"B-1003\", \"guest_name\": \"Nina\",  \"revenue\": 90.0,  \"room_type\": \"Standard\"},\n",
    "    {\"booking_id\": \"B-1004\", \"guest_name\": \"Budi\",  \"revenue\": 250.0, \"room_type\": \"Suite\"},\n",
    "    {\"booking_id\": \"B-1005\", \"guest_name\": \"Sari\",  \"revenue\": 110.0, \"room_type\": \"Deluxe\"},\n",
    "]\n",
    "\n",
    "# open() with newline='' is REQUIRED for csv module on all platforms\n",
    "# to prevent extra blank lines between rows.\n",
    "with csv_path.open(\"w\", newline=\"\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=rows[0].keys())\n",
    "    writer.writeheader()   # writes: booking_id,guest_name,revenue,room_type\n",
    "    writer.writerows(rows) # writes all dicts as CSV rows\n",
    "\n",
    "print(f\"Wrote {len(rows)} rows → {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 2b. Reading CSV with DictReader\n",
    "# ============================================================\n",
    "# DictReader reads each row as an OrderedDict (key = column name).\n",
    "# IMPORTANT: All values come back as STRINGS — you must cast manually.\n",
    "\n",
    "with csv_path.open(\"r\", newline=\"\") as f:\n",
    "    # DictReader auto-detects the header from the first row\n",
    "    reader = csv.DictReader(f)\n",
    "\n",
    "    print(f\"Detected columns: {reader.fieldnames}\\n\")\n",
    "\n",
    "    for row in reader:\n",
    "        # row['revenue'] is a STRING like '120.0', so we cast to float\n",
    "        revenue = float(row[\"revenue\"])\n",
    "        print(f\"  {row['booking_id']} | {row['guest_name']:>6} | ${revenue:>7.2f} | {row['room_type']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 2c. Custom Delimiters & Quoting\n",
    "# ============================================================\n",
    "# Real-world data isn't always comma-separated.\n",
    "# The csv module supports custom delimiters (tab, pipe, etc.)\n",
    "# and quoting strategies for fields that contain special characters.\n",
    "\n",
    "tsv_path = Path(\"data\") / \"output\" / \"bookings.tsv\"\n",
    "\n",
    "with tsv_path.open(\"w\", newline=\"\") as f:\n",
    "    writer = csv.DictWriter(\n",
    "        f,\n",
    "        fieldnames=rows[0].keys(),\n",
    "        delimiter=\"\\t\",                  # use TAB instead of comma\n",
    "        quoting=csv.QUOTE_NONNUMERIC,    # quote all non-numeric fields\n",
    "        # Quoting options:\n",
    "        #   csv.QUOTE_MINIMAL    → only quote fields containing the delimiter (default)\n",
    "        #   csv.QUOTE_ALL        → quote every field\n",
    "        #   csv.QUOTE_NONNUMERIC → quote strings, leave numbers unquoted\n",
    "        #   csv.QUOTE_NONE       → never quote (use escapechar for special chars)\n",
    "    )\n",
    "    writer.writeheader()\n",
    "    writer.writerows(rows)\n",
    "\n",
    "# Read back the TSV and display\n",
    "print(\"--- TSV Content ---\")\n",
    "print(tsv_path.read_text()[:400])  # read_text() reads entire file as a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 2d. Reading with csv.reader (list-based, no header mapping)\n",
    "# ============================================================\n",
    "# csv.reader returns each row as a plain list of strings.\n",
    "# Useful when you want positional access or the file has no header.\n",
    "\n",
    "with csv_path.open(\"r\", newline=\"\") as f:\n",
    "    reader = csv.reader(f)\n",
    "\n",
    "    # next() advances the iterator by one step — here it skips the header row\n",
    "    header = next(reader)\n",
    "    print(\"Header:\", header)\n",
    "\n",
    "    # enumerate() gives (index, value) pairs — handy for row numbering\n",
    "    for i, row in enumerate(reader, start=1):\n",
    "        print(f\"  Row {i}: {row}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. JSON & JSONL (JSON Lines)\n",
    "\n",
    "- **JSON**: Great for nested/hierarchical data (configs, API responses).  \n",
    "- **JSONL** (JSON Lines): One JSON object per line — perfect for **streaming** large datasets.  \n",
    "  Each line is independently parseable, so you can process files line-by-line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# ============================================================\n",
    "# 3a. Standard JSON — read / write\n",
    "# ============================================================\n",
    "\n",
    "json_path = Path(\"data\") / \"bookings.json\"\n",
    "\n",
    "# Build a nested payload (typical API / config structure)\n",
    "payload = {\n",
    "    \"property_id\": \"HTL-01\",\n",
    "    \"property_name\": \"Grand Hotel Jakarta\",\n",
    "    \"bookings\": rows,   # our list of dicts from section 2\n",
    "}\n",
    "\n",
    "# --- Writing ---\n",
    "# json.dumps() converts a Python object → JSON string\n",
    "#   indent=2       → pretty-print with 2-space indentation\n",
    "#   ensure_ascii=False → allow non-ASCII chars (e.g. accented names)\n",
    "json_text = json.dumps(payload, indent=2, ensure_ascii=False)\n",
    "json_path.write_text(json_text)   # Path.write_text() writes a string to a file\n",
    "\n",
    "print(f\"Wrote JSON → {json_path}\")\n",
    "print(json_text[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Reading ---\n",
    "# json.loads() converts a JSON string → Python object (dict/list)\n",
    "loaded = json.loads(json_path.read_text())\n",
    "\n",
    "print(f\"Property : {loaded['property_name']}\")\n",
    "print(f\"# bookings: {len(loaded['bookings'])}\")\n",
    "print(f\"First booking: {loaded['bookings'][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3b. JSONL (JSON Lines) — write\n",
    "# ============================================================\n",
    "# JSONL = one JSON object per line, NO wrapping array.\n",
    "# Format:\n",
    "#   {\"booking_id\": \"B-1001\", \"guest_name\": \"Alya\", ...}\n",
    "#   {\"booking_id\": \"B-1002\", \"guest_name\": \"Rafi\", ...}\n",
    "#\n",
    "# Why JSONL?\n",
    "#   - Appendable: just add a new line (no need to rewrite the whole file)\n",
    "#   - Streamable: process one record at a time (low memory)\n",
    "#   - Used by: BigQuery, Spark, many ETL pipelines\n",
    "\n",
    "jsonl_path = Path(\"data\") / \"output\" / \"bookings.jsonl\"\n",
    "\n",
    "with jsonl_path.open(\"w\") as f:\n",
    "    for record in rows:\n",
    "        # json.dumps() with NO indent → single-line JSON (required for JSONL)\n",
    "        line = json.dumps(record, ensure_ascii=False)\n",
    "        f.write(line + \"\\n\")  # each record on its own line\n",
    "\n",
    "print(f\"Wrote {len(rows)} records → {jsonl_path}\")\n",
    "print(\"\\n--- JSONL Content ---\")\n",
    "print(jsonl_path.read_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3c. JSONL — read (streaming, line-by-line)\n",
    "# ============================================================\n",
    "# Reading JSONL is simple: iterate lines, parse each independently.\n",
    "# This approach uses almost NO memory regardless of file size.\n",
    "\n",
    "print(\"--- Reading JSONL records ---\")\n",
    "with jsonl_path.open(\"r\") as f:\n",
    "    for line_number, line in enumerate(f, start=1):\n",
    "        # strip() removes leading/trailing whitespace and the newline\n",
    "        line = line.strip()\n",
    "        if not line:          # skip empty lines (defensive)\n",
    "            continue\n",
    "        record = json.loads(line)   # parse ONE JSON object\n",
    "        print(f\"  Record {line_number}: {record['booking_id']} — {record['guest_name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Advanced Context Managers\n",
    "\n",
    "The `with` statement ensures resources (files, connections, locks) are **always cleaned up**,  \n",
    "even if an exception occurs. You can build your own context managers two ways:\n",
    "\n",
    "1. **Class-based** — implement `__enter__` and `__exit__`  \n",
    "2. **Generator-based** — use `@contextmanager` from `contextlib`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 4a. Class-Based Context Manager\n",
    "# ============================================================\n",
    "# __enter__() is called when entering the `with` block → returns the resource\n",
    "# __exit__()  is called when LEAVING the `with` block (even on exception)\n",
    "#   exc_type, exc_val, exc_tb → exception info (None if no error)\n",
    "#   return True to SUPPRESS the exception, False/None to let it propagate\n",
    "\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "class CSVBatchWriter:\n",
    "    \"\"\"\n",
    "    Context manager that collects rows in a buffer and writes them\n",
    "    to CSV in one batch when the `with` block exits.\n",
    "\n",
    "    Usage:\n",
    "        with CSVBatchWriter('out.csv', ['col_a', 'col_b']) as writer:\n",
    "            writer.add({'col_a': 1, 'col_b': 2})\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path: str, fieldnames: list[str]):\n",
    "        self.path = Path(path)\n",
    "        self.fieldnames = fieldnames\n",
    "        self.buffer: list[dict] = []     # in-memory row buffer\n",
    "\n",
    "    def __enter__(self):\n",
    "        \"\"\"Called on entering `with`. Returns self so we can call .add().\"\"\"\n",
    "        print(f\"[CSVBatchWriter] Opened — buffering rows for {self.path.name}\")\n",
    "        return self\n",
    "\n",
    "    def add(self, row: dict):\n",
    "        \"\"\"Append a row to the in-memory buffer (not written to disk yet).\"\"\"\n",
    "        self.buffer.append(row)\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        \"\"\"\n",
    "        Called on leaving `with`. Flushes all buffered rows to disk.\n",
    "        Always runs — even if an exception occurred in the `with` block.\n",
    "        \"\"\"\n",
    "        with self.path.open(\"w\", newline=\"\") as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=self.fieldnames)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(self.buffer)\n",
    "        print(f\"[CSVBatchWriter] Flushed {len(self.buffer)} rows → {self.path}\")\n",
    "        return False   # don't suppress exceptions\n",
    "\n",
    "\n",
    "# --- Demo ---\n",
    "batch_csv = Path(\"data\") / \"output\" / \"batch_bookings.csv\"\n",
    "\n",
    "with CSVBatchWriter(str(batch_csv), [\"booking_id\", \"guest_name\", \"revenue\"]) as writer:\n",
    "    writer.add({\"booking_id\": \"B-2001\", \"guest_name\": \"Dewi\",  \"revenue\": 200.0})\n",
    "    writer.add({\"booking_id\": \"B-2002\", \"guest_name\": \"Agus\",  \"revenue\": 150.0})\n",
    "    writer.add({\"booking_id\": \"B-2003\", \"guest_name\": \"Rina\",  \"revenue\": 320.0})\n",
    "\n",
    "# Verify the file was written\n",
    "print(\"\\n--- Batch CSV Content ---\")\n",
    "print(batch_csv.read_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 4b. Generator-Based Context Manager (@contextmanager)\n",
    "# ============================================================\n",
    "# contextlib.contextmanager turns a generator function into a context manager.\n",
    "# Everything BEFORE `yield` = setup (__enter__)\n",
    "# The yielded value          = what `as` receives\n",
    "# Everything AFTER  `yield`  = teardown (__exit__)\n",
    "\n",
    "from contextlib import contextmanager\n",
    "import time\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def timer(label: str):\n",
    "    \"\"\"\n",
    "    A simple timer context manager.\n",
    "    Measures elapsed wall-clock time for the code inside the `with` block.\n",
    "    \"\"\"\n",
    "    # --- SETUP (runs before the `with` body) ---\n",
    "    start = time.perf_counter()  # perf_counter() returns high-resolution time in seconds\n",
    "    print(f\"[Timer] {label} — started\")\n",
    "\n",
    "    yield  # <-- control goes to the `with` block here\n",
    "\n",
    "    # --- TEARDOWN (runs after the `with` body) ---\n",
    "    elapsed = time.perf_counter() - start\n",
    "    print(f\"[Timer] {label} — finished in {elapsed:.4f}s\")\n",
    "\n",
    "\n",
    "# --- Demo ---\n",
    "with timer(\"Read hotel CSV headers\"):\n",
    "    hotel_csv = Path(\"data/hotel_booking.csv\")\n",
    "    with hotel_csv.open(\"r\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        header = next(reader)   # read only the first line\n",
    "    print(f\"  Found {len(header)} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 4c. Practical Example — Temporary Working File\n",
    "# ============================================================\n",
    "# A context manager that creates a temp file for processing,\n",
    "# then cleans it up automatically when done.\n",
    "\n",
    "@contextmanager\n",
    "def temp_working_file(path: str):\n",
    "    \"\"\"\n",
    "    Creates a temporary file at `path`. Yields the Path object.\n",
    "    Automatically deletes the file when the `with` block exits.\n",
    "    \"\"\"\n",
    "    p = Path(path)\n",
    "    p.parent.mkdir(parents=True, exist_ok=True)\n",
    "    try:\n",
    "        yield p             # hand the path to the caller\n",
    "    finally:\n",
    "        # finally block runs even if an exception occurs\n",
    "        if p.exists():\n",
    "            p.unlink()      # unlink() deletes the file\n",
    "            print(f\"[Cleanup] Deleted temp file: {p}\")\n",
    "\n",
    "\n",
    "# --- Demo ---\n",
    "with temp_working_file(\"data/output/temp_staging.csv\") as tmp:\n",
    "    tmp.write_text(\"id,value\\n1,100\\n2,200\\n\")\n",
    "    print(f\"Temp file exists: {tmp.exists()}\")\n",
    "    print(f\"Content: {tmp.read_text()}\")\n",
    "\n",
    "# After the `with` block, the file is gone\n",
    "print(f\"Temp file exists after block: {Path('data/output/temp_staging.csv').exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Generators — Memory-Efficient Large File Processing\n",
    "\n",
    "A **generator** is a function that uses `yield` instead of `return`.  \n",
    "It produces values **one at a time** (lazy evaluation), so it never loads the entire dataset into RAM.\n",
    "\n",
    "This is critical for Data Engineering where files can be **gigabytes** in size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 5a. Generator Basics — yield vs return\n",
    "# ============================================================\n",
    "\n",
    "def count_up_to(n: int):\n",
    "    \"\"\"\n",
    "    A generator that yields numbers 1 through n.\n",
    "\n",
    "    Unlike a regular function that builds a list in memory,\n",
    "    this produces ONE value at a time and pauses between yields.\n",
    "    \"\"\"\n",
    "    i = 1\n",
    "    while i <= n:\n",
    "        yield i    # pause here, return `i` to caller, resume on next iteration\n",
    "        i += 1\n",
    "\n",
    "\n",
    "# The generator is LAZY — nothing runs until you iterate\n",
    "gen = count_up_to(5)\n",
    "print(\"Type:\", type(gen))     # <class 'generator'>\n",
    "print(\"next():\", next(gen))   # 1  — next() advances the generator by one step\n",
    "print(\"next():\", next(gen))   # 2\n",
    "\n",
    "# for-loop consumes the rest\n",
    "print(\"Remaining:\", list(gen))  # [3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 5b. Generator for Large CSV — stream rows without loading all into RAM\n",
    "# ============================================================\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def stream_csv(filepath: str | Path, encoding: str = \"utf-8\"):\n",
    "    \"\"\"\n",
    "    Generator that yields one dict per row from a CSV file.\n",
    "    Memory usage is O(1) regardless of file size — only one row in memory at a time.\n",
    "\n",
    "    Args:\n",
    "        filepath: Path to the CSV file\n",
    "        encoding: Text encoding (default: utf-8)\n",
    "\n",
    "    Yields:\n",
    "        dict — one row as {column_name: value}\n",
    "    \"\"\"\n",
    "    with open(filepath, \"r\", encoding=encoding, newline=\"\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            yield row   # yield ONE row, then pause until next() is called\n",
    "\n",
    "\n",
    "# --- Demo: process 119K-row hotel_booking.csv without loading it all ---\n",
    "hotel_csv = Path(\"data/hotel_booking.csv\")\n",
    "\n",
    "# Count rows and sum revenue — streaming, constant memory\n",
    "total_rows = 0\n",
    "total_adr = 0.0     # ADR = Average Daily Rate\n",
    "\n",
    "for row in stream_csv(hotel_csv):\n",
    "    total_rows += 1\n",
    "    # adr column might be empty or non-numeric; handle gracefully\n",
    "    try:\n",
    "        total_adr += float(row[\"adr\"])\n",
    "    except (ValueError, KeyError):\n",
    "        pass\n",
    "\n",
    "print(f\"Total rows    : {total_rows:,}\")\n",
    "print(f\"Sum of ADR    : ${total_adr:,.2f}\")\n",
    "print(f\"Average ADR   : ${total_adr / total_rows:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 5c. Generator Pipeline — chain generators for complex processing\n",
    "# ============================================================\n",
    "# Generators can be COMPOSED: output of one feeds into the next.\n",
    "# Each step processes one record at a time → entire pipeline is O(1) memory.\n",
    "\n",
    "\n",
    "def filter_rows(rows, column: str, value: str):\n",
    "    \"\"\"\n",
    "    Generator that yields only rows where row[column] == value.\n",
    "    Like SQL: WHERE column = value\n",
    "    \"\"\"\n",
    "    for row in rows:\n",
    "        if row[column] == value:\n",
    "            yield row\n",
    "\n",
    "\n",
    "def select_columns(rows, columns: list[str]):\n",
    "    \"\"\"\n",
    "    Generator that yields dicts with only the specified columns.\n",
    "    Like SQL: SELECT col1, col2 FROM ...\n",
    "    \"\"\"\n",
    "    for row in rows:\n",
    "        # dict comprehension: build a new dict with only desired keys\n",
    "        yield {col: row[col] for col in columns}\n",
    "\n",
    "\n",
    "def add_computed_column(rows, new_col: str, func):\n",
    "    \"\"\"\n",
    "    Generator that adds a new computed column to each row.\n",
    "    Like SQL: SELECT *, func(...) AS new_col FROM ...\n",
    "    \"\"\"\n",
    "    for row in rows:\n",
    "        row[new_col] = func(row)   # mutate the dict in-place\n",
    "        yield row\n",
    "\n",
    "\n",
    "# --- Build a pipeline: stream → filter → select → compute → collect ---\n",
    "pipeline = stream_csv(hotel_csv)                                       # source\n",
    "pipeline = filter_rows(pipeline, \"hotel\", \"Resort Hotel\")              # WHERE\n",
    "pipeline = filter_rows(pipeline, \"is_canceled\", \"0\")                   # AND\n",
    "pipeline = select_columns(pipeline, [\"hotel\", \"adr\", \"country\"])       # SELECT\n",
    "pipeline = add_computed_column(                                        # computed col\n",
    "    pipeline,\n",
    "    \"adr_category\",\n",
    "    lambda r: \"Premium\" if float(r[\"adr\"]) > 150 else \"Standard\",\n",
    ")\n",
    "\n",
    "# Consume only the first 5 results (the rest are never even read from disk!)\n",
    "from itertools import islice    # islice() takes a slice of any iterable (like list[:5])\n",
    "\n",
    "print(\"--- Generator Pipeline: Resort Hotel, not canceled, first 5 ---\")\n",
    "for record in islice(pipeline, 5):\n",
    "    print(f\"  {record}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 5d. Generator to JSONL — stream large CSV to JSONL format\n",
    "# ============================================================\n",
    "# A common DE task: convert CSV → JSONL for downstream systems.\n",
    "# Using generators, this works on files of ANY size.\n",
    "\n",
    "import json\n",
    "\n",
    "def csv_to_jsonl(csv_path: str | Path, jsonl_path: str | Path, limit: int | None = None):\n",
    "    \"\"\"\n",
    "    Streams a CSV file and writes it as JSONL.\n",
    "    Memory usage: O(1) — one row at a time.\n",
    "\n",
    "    Args:\n",
    "        csv_path  : source CSV file\n",
    "        jsonl_path: destination JSONL file\n",
    "        limit     : max rows to convert (None = all)\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    with open(jsonl_path, \"w\") as out:\n",
    "        for row in stream_csv(csv_path):\n",
    "            out.write(json.dumps(row) + \"\\n\")\n",
    "            count += 1\n",
    "            if limit and count >= limit:\n",
    "                break\n",
    "    return count\n",
    "\n",
    "\n",
    "# Convert first 1000 rows of hotel_booking.csv → JSONL\n",
    "output_jsonl = Path(\"data/output/hotel_sample.jsonl\")\n",
    "n = csv_to_jsonl(hotel_csv, output_jsonl, limit=1000)\n",
    "print(f\"Converted {n} rows → {output_jsonl}\")\n",
    "print(f\"File size: {output_jsonl.stat().st_size / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Binary & Buffered Reading\n",
    "\n",
    "Sometimes you need to read files as **raw bytes** instead of text:  \n",
    "- Calculating checksums or file hashes  \n",
    "- Inspecting file headers (magic bytes)  \n",
    "- Processing binary formats (images, protobuf)  \n",
    "- Counting bytes for progress bars on large files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 6a. Reading in Byte Chunks\n",
    "# ============================================================\n",
    "# open() with mode='rb' opens a file in BINARY mode.\n",
    "# f.read(chunk_size) reads up to `chunk_size` bytes at a time.\n",
    "# Returns b'' (empty bytes) when the file is fully read.\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "hotel_csv = Path(\"data/hotel_booking.csv\")\n",
    "chunk_size = 64 * 1024   # 64 KB chunks — a common buffer size\n",
    "\n",
    "total_bytes = 0\n",
    "chunk_count = 0\n",
    "\n",
    "with hotel_csv.open(\"rb\") as f:        # 'rb' = read binary\n",
    "    while True:\n",
    "        chunk = f.read(chunk_size)      # read up to 64KB\n",
    "        if not chunk:                   # empty bytes = end of file\n",
    "            break\n",
    "        total_bytes += len(chunk)       # len() on bytes = byte count\n",
    "        chunk_count += 1\n",
    "\n",
    "file_size = hotel_csv.stat().st_size\n",
    "print(f\"File size (stat) : {file_size:,} bytes\")\n",
    "print(f\"Total bytes read : {total_bytes:,} bytes\")\n",
    "print(f\"Chunks read      : {chunk_count}\")\n",
    "print(f\"Match            : {total_bytes == file_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 6b. File Hashing (Checksum) — verify data integrity\n",
    "# ============================================================\n",
    "# hashlib provides cryptographic hash functions (MD5, SHA-256, etc.).\n",
    "# Hashing a file in chunks avoids loading it entirely into memory.\n",
    "#\n",
    "# Use case: verify a downloaded dataset hasn't been corrupted.\n",
    "\n",
    "import hashlib\n",
    "\n",
    "\n",
    "def file_hash(filepath: str | Path, algorithm: str = \"sha256\", chunk_size: int = 65536) -> str:\n",
    "    \"\"\"\n",
    "    Compute the hash of a file by reading it in chunks.\n",
    "    Memory: O(chunk_size), NOT O(file_size).\n",
    "\n",
    "    Args:\n",
    "        filepath  : path to the file\n",
    "        algorithm : hash algorithm name (e.g. 'md5', 'sha256')\n",
    "        chunk_size: bytes per read (default 64KB)\n",
    "\n",
    "    Returns:\n",
    "        Hex digest string of the hash\n",
    "    \"\"\"\n",
    "    # hashlib.new() creates a hash object for the given algorithm\n",
    "    h = hashlib.new(algorithm)\n",
    "\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        while True:\n",
    "            chunk = f.read(chunk_size)\n",
    "            if not chunk:\n",
    "                break\n",
    "            h.update(chunk)  # update() feeds bytes into the hash incrementally\n",
    "\n",
    "    return h.hexdigest()     # hexdigest() returns the final hash as a hex string\n",
    "\n",
    "\n",
    "# --- Demo ---\n",
    "sha = file_hash(hotel_csv, \"sha256\")\n",
    "md5 = file_hash(hotel_csv, \"md5\")\n",
    "\n",
    "print(f\"SHA-256 : {sha}\")\n",
    "print(f\"MD5     : {md5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 6c. Counting Lines Efficiently with Binary Read\n",
    "# ============================================================\n",
    "# Counting lines by reading text (for line in f) is fine, but\n",
    "# counting newline BYTES in binary chunks is often faster for huge files.\n",
    "\n",
    "\n",
    "def count_lines_binary(filepath: str | Path, chunk_size: int = 65536) -> int:\n",
    "    \"\"\"\n",
    "    Count the number of lines in a file using binary chunk reading.\n",
    "    Faster than text-mode iteration for very large files.\n",
    "\n",
    "    Counts occurrences of the newline byte (b'\\\\n') in each chunk.\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        while True:\n",
    "            chunk = f.read(chunk_size)\n",
    "            if not chunk:\n",
    "                break\n",
    "            count += chunk.count(b\"\\n\")  # bytes.count() counts occurrences of a byte pattern\n",
    "    return count\n",
    "\n",
    "\n",
    "# --- Demo: count lines in hotel_booking.csv ---\n",
    "with timer(\"Binary line count\"):\n",
    "    n_lines = count_lines_binary(hotel_csv)\n",
    "    print(f\"  Lines (including header): {n_lines:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Mini Pipeline — Putting It All Together\n",
    "\n",
    "Let's combine everything: **Pathlib + CSV streaming + generators + JSONL output**  \n",
    "to build a mini ETL pipeline on the `hotel_booking.csv` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 7. Full Pipeline: hotel_booking.csv → filtered JSONL summary\n",
    "# ============================================================\n",
    "# Task:\n",
    "#   1. Stream the 119K-row hotel CSV\n",
    "#   2. Filter to non-canceled bookings at \"City Hotel\"\n",
    "#   3. Extract & transform selected columns\n",
    "#   4. Write results as JSONL\n",
    "#   5. Print summary stats\n",
    "\n",
    "import csv\n",
    "import json\n",
    "from pathlib import Path\n",
    "from contextlib import contextmanager\n",
    "import time\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def timer(label: str):\n",
    "    \"\"\"Reusable timer context manager.\"\"\"\n",
    "    start = time.perf_counter()\n",
    "    yield\n",
    "    elapsed = time.perf_counter() - start\n",
    "    print(f\"[{label}] completed in {elapsed:.4f}s\")\n",
    "\n",
    "\n",
    "# --- Generator pipeline functions ---\n",
    "\n",
    "def stream_csv(filepath):\n",
    "    \"\"\"Yield one dict per CSV row (O(1) memory).\"\"\"\n",
    "    with open(filepath, \"r\", newline=\"\") as f:\n",
    "        for row in csv.DictReader(f):\n",
    "            yield row\n",
    "\n",
    "\n",
    "def transform_booking(row: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Transform a raw hotel booking row into a clean summary record.\n",
    "    Handles type casting and derives new fields.\n",
    "    \"\"\"\n",
    "    adr = float(row.get(\"adr\", 0) or 0)  # Average Daily Rate; handle empty strings\n",
    "    weekend = int(row.get(\"stays_in_weekend_nights\", 0) or 0)\n",
    "    weekday = int(row.get(\"stays_in_week_nights\", 0) or 0)\n",
    "    total_nights = weekend + weekday\n",
    "\n",
    "    return {\n",
    "        \"country\": row.get(\"country\", \"UNKNOWN\"),\n",
    "        \"room_type\": row.get(\"reserved_room_type\", \"?\"),\n",
    "        \"adr\": adr,\n",
    "        \"total_nights\": total_nights,\n",
    "        \"total_revenue\": round(adr * total_nights, 2),  # round() avoids floating-point noise\n",
    "        \"arrival_year\": row.get(\"arrival_date_year\", \"\"),\n",
    "        \"arrival_month\": row.get(\"arrival_date_month\", \"\"),\n",
    "    }\n",
    "\n",
    "\n",
    "# --- Execute the pipeline ---\n",
    "\n",
    "source = Path(\"data/hotel_booking.csv\")\n",
    "output = Path(\"data/output/city_hotel_bookings.jsonl\")\n",
    "output.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with timer(\"Full pipeline\"):\n",
    "    written = 0\n",
    "    total_revenue = 0.0\n",
    "    country_counts: dict[str, int] = {}  # manual counter (no collections import needed)\n",
    "\n",
    "    with output.open(\"w\") as out_file:\n",
    "        for raw_row in stream_csv(source):\n",
    "            # --- Filter: City Hotel + not canceled ---\n",
    "            if raw_row[\"hotel\"] != \"City Hotel\":\n",
    "                continue\n",
    "            if raw_row[\"is_canceled\"] != \"0\":\n",
    "                continue\n",
    "\n",
    "            # --- Transform ---\n",
    "            record = transform_booking(raw_row)\n",
    "\n",
    "            # --- Accumulate stats ---\n",
    "            total_revenue += record[\"total_revenue\"]\n",
    "            country = record[\"country\"]\n",
    "            country_counts[country] = country_counts.get(country, 0) + 1\n",
    "\n",
    "            # --- Write JSONL ---\n",
    "            out_file.write(json.dumps(record) + \"\\n\")\n",
    "            written += 1\n",
    "\n",
    "# --- Summary ---\n",
    "print(f\"\\nRecords written : {written:,}\")\n",
    "print(f\"Total revenue   : ${total_revenue:,.2f}\")\n",
    "print(f\"Output file     : {output} ({output.stat().st_size / 1024:.1f} KB)\")\n",
    "print(f\"Output SHA-256  : {file_hash(output)}\")\n",
    "\n",
    "# Top 5 countries by booking count\n",
    "# sorted() with key=lambda sorts by value descending\n",
    "top_countries = sorted(country_counts.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "print(f\"\\nTop 5 countries:\")\n",
    "for country, count in top_countries:\n",
    "    print(f\"  {country:>5} : {count:,} bookings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick peek at the first 3 output records\n",
    "print(\"--- First 3 JSONL records ---\")\n",
    "with output.open(\"r\") as f:\n",
    "    for line in __import__(\"itertools\").islice(f, 3):\n",
    "        # json.loads → parse,  json.dumps(indent=2) → pretty-print\n",
    "        record = json.loads(line)\n",
    "        print(json.dumps(record, indent=2))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Key Takeaways\n",
    "\n",
    "| Concept | Why it matters for DE |\n",
    "|---------|----------------------|\n",
    "| **Pathlib** | Clean, cross-platform file paths — no more `os.path.join()` chains |\n",
    "| **csv module** | Stream CSV row-by-row with full control over delimiters and quoting |\n",
    "| **JSON / JSONL** | JSONL is the go-to format for streaming structured data in pipelines |\n",
    "| **Context Managers** | Guarantee cleanup of files, connections, and temp resources |\n",
    "| **Generators** | Process files of ANY size in constant memory with `yield` |\n",
    "| **Binary reading** | Hash files, count lines fast, handle non-text formats |\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Continue with **`03b_standard_libs.ipynb`** — Python's built-in libraries for datetime, logging, collections, and more."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

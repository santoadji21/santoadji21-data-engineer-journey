{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 — Numpy, Pandas & Parquet Deep Dive\n",
    "\n",
    "Master the core data processing stack every data engineer relies on.\n",
    "\n",
    "### What you'll learn\n",
    "| # | Topic | Key Takeaway |\n",
    "|---|-------|--------------|\n",
    "| 1 | **Numpy** | Arrays, vectorization, `np.where`, `np.select`, `np.unique`, `np.isnan` |\n",
    "| 2 | **Pandas IO** | `read_csv`, `read_parquet`, `to_parquet` — loading & saving data |\n",
    "| 3 | **Pandas Transformation** | `apply`, `map`, `astype` — reshaping values |\n",
    "| 4 | **Pandas Cleaning** | `fillna`, `dropna`, `drop_duplicates` — data quality |\n",
    "| 5 | **Pandas Joins** | `merge` (SQL-style), `concat` (stacking) |\n",
    "| 6 | **Pandas Aggregations** | `groupby`, `agg`, `pivot_table` — summarizing data |\n",
    "| 7 | **Parquet** | Columnar storage, compression, and efficient IO |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Numpy — The Engine Behind Pandas\n",
    "\n",
    "Numpy provides **fixed-type arrays** and **vectorized operations** (no Python loops).  \n",
    "Pandas DataFrames are built on top of Numpy arrays, so understanding Numpy = understanding Pandas performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ============================================================\n",
    "# 1a. Array Basics\n",
    "# ============================================================\n",
    "\n",
    "# np.array() creates an array from a Python list.\n",
    "# Unlike lists, ALL elements must be the same type → enables fast math.\n",
    "prices = np.array([120.0, 180.0, 90.0, 200.0, 160.0])\n",
    "\n",
    "print(f\"Array  : {prices}\")\n",
    "print(f\"Type   : {type(prices)}\")         # numpy.ndarray\n",
    "print(f\"dtype  : {prices.dtype}\")          # float64 — the element data type\n",
    "print(f\"shape  : {prices.shape}\")          # (5,) — dimensions (rows,) for 1D\n",
    "print(f\"ndim   : {prices.ndim}\")           # 1 — number of dimensions\n",
    "print(f\"size   : {prices.size}\")           # 5 — total element count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1b. Vectorization — why Numpy is fast\n",
    "# ============================================================\n",
    "# \"Vectorized\" means the operation is applied to ALL elements at once\n",
    "# in compiled C code — no Python for-loop needed.\n",
    "\n",
    "nights = np.array([2, 3, 1, 4, 2])\n",
    "\n",
    "# Element-wise math — each element is processed in parallel\n",
    "revenue_per_night = prices / nights        # divide each price by its night count\n",
    "discounted = prices * 0.9                  # 10% discount on all prices\n",
    "total_with_tax = prices * 1.11             # 11% tax added\n",
    "\n",
    "print(f\"Prices          : {prices}\")\n",
    "print(f\"Nights          : {nights}\")\n",
    "print(f\"Rev/night       : {revenue_per_night}\")\n",
    "print(f\"10% discount    : {discounted}\")\n",
    "print(f\"With 11% tax    : {total_with_tax}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1c. Aggregation Functions\n",
    "# ============================================================\n",
    "# These reduce an array to a single value (or along an axis).\n",
    "\n",
    "print(f\"sum()   : {prices.sum()}\")     # sum of all elements\n",
    "print(f\"mean()  : {prices.mean()}\")    # arithmetic average\n",
    "print(f\"std()   : {prices.std():.2f}\") # standard deviation\n",
    "print(f\"min()   : {prices.min()}\")     # minimum value\n",
    "print(f\"max()   : {prices.max()}\")     # maximum value\n",
    "print(f\"argmin(): {prices.argmin()}\")  # INDEX of the minimum value\n",
    "print(f\"argmax(): {prices.argmax()}\")  # INDEX of the maximum value\n",
    "\n",
    "# cumsum() returns a running total (cumulative sum)\n",
    "print(f\"cumsum(): {prices.cumsum()}\")  # [120, 300, 390, 590, 750]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1d. Boolean Indexing (Filtering)\n",
    "# ============================================================\n",
    "# Comparison operators on arrays return boolean arrays.\n",
    "# Use boolean arrays as a MASK to select elements.\n",
    "\n",
    "mask = prices > 150          # element-wise comparison → [False, True, False, True, True]\n",
    "print(f\"Mask (>150)     : {mask}\")\n",
    "print(f\"Premium prices  : {prices[mask]}\")  # only elements where mask is True\n",
    "\n",
    "# Combine conditions with & (and), | (or), ~ (not)\n",
    "# IMPORTANT: wrap each condition in parentheses!\n",
    "mid_range = prices[(prices >= 100) & (prices <= 180)]\n",
    "print(f\"Mid-range (100-180): {mid_range}\")\n",
    "\n",
    "# Count matches\n",
    "n_premium = (prices > 150).sum()  # True=1, False=0, so sum() counts Trues\n",
    "print(f\"Count > 150     : {n_premium}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1e. np.where — vectorized if-else\n",
    "# ============================================================\n",
    "# np.where(condition, value_if_true, value_if_false)\n",
    "# Like: [true_val if cond else false_val for each element]\n",
    "# But runs in C — orders of magnitude faster.\n",
    "\n",
    "# Classify each price as \"Premium\" or \"Standard\"\n",
    "categories = np.where(prices > 150, \"Premium\", \"Standard\")\n",
    "print(f\"Prices     : {prices}\")\n",
    "print(f\"Categories : {categories}\")\n",
    "\n",
    "# Numeric example: cap prices at 180 (any price above 180 → 180)\n",
    "capped = np.where(prices > 180, 180, prices)\n",
    "print(f\"Capped     : {capped}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1f. np.select — multiple conditions (like CASE WHEN in SQL)\n",
    "# ============================================================\n",
    "# np.select(condlist, choicelist, default)\n",
    "#   condlist  : list of boolean arrays (conditions)\n",
    "#   choicelist: list of values (one per condition)\n",
    "#   default   : value when no condition is True\n",
    "#\n",
    "# Equivalent SQL:\n",
    "#   CASE WHEN price > 200 THEN 'Luxury'\n",
    "#        WHEN price > 120 THEN 'Premium'\n",
    "#        ELSE 'Budget' END\n",
    "\n",
    "conditions = [\n",
    "    prices > 200,     # condition 1\n",
    "    prices > 120,     # condition 2\n",
    "]\n",
    "choices = [\n",
    "    \"Luxury\",         # if condition 1 is True\n",
    "    \"Premium\",        # if condition 2 is True (and condition 1 is False)\n",
    "]\n",
    "\n",
    "tiers = np.select(conditions, choices, default=\"Budget\")\n",
    "print(f\"Prices : {prices}\")\n",
    "print(f\"Tiers  : {tiers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1g. np.unique — find distinct values\n",
    "# ============================================================\n",
    "# np.unique(array) returns sorted unique values.\n",
    "# Like SQL: SELECT DISTINCT ... ORDER BY ...\n",
    "\n",
    "countries = np.array([\"PRT\", \"GBR\", \"FRA\", \"PRT\", \"GBR\", \"PRT\", \"DEU\", \"FRA\"])\n",
    "\n",
    "# Basic unique values\n",
    "unique_countries = np.unique(countries)\n",
    "print(f\"Unique : {unique_countries}\")\n",
    "\n",
    "# return_counts=True also gives the count of each unique value\n",
    "values, counts = np.unique(countries, return_counts=True)\n",
    "print(\"\\nValue counts:\")\n",
    "for v, c in zip(values, counts):  # zip() pairs elements from two iterables\n",
    "    print(f\"  {v}: {c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1h. np.isnan — detect missing/NaN values\n",
    "# ============================================================\n",
    "# NaN (Not a Number) = Numpy/Pandas representation of missing data.\n",
    "# You CANNOT check NaN with == (NaN != NaN is True by IEEE standard).\n",
    "# Use np.isnan() instead.\n",
    "\n",
    "data = np.array([120.0, np.nan, 90.0, np.nan, 160.0])\n",
    "\n",
    "print(f\"Data         : {data}\")\n",
    "print(f\"isnan mask   : {np.isnan(data)}\")       # [False, True, False, True, False]\n",
    "print(f\"Count NaN    : {np.isnan(data).sum()}\") # 2\n",
    "\n",
    "# np.nanmean / np.nansum — aggregate IGNORING NaN values\n",
    "print(f\"mean (w/ NaN): {np.mean(data)}\")     # nan (any NaN poisons the result)\n",
    "print(f\"nanmean      : {np.nanmean(data)}\")  # 123.33 (ignores NaN)\n",
    "print(f\"nansum       : {np.nansum(data)}\")   # 370.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Pandas IO — Loading & Saving Data\n",
    "\n",
    "Pandas is the workhorse of data processing in Python.  \n",
    "It adds **labeled columns**, **mixed types**, and **rich IO** on top of Numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ============================================================\n",
    "# 2a. read_csv — load CSV into a DataFrame\n",
    "# ============================================================\n",
    "# pd.read_csv() is the most common way to load data.\n",
    "# It returns a DataFrame — a 2D labeled table with typed columns.\n",
    "\n",
    "hotel_csv = Path(\"data/hotel_booking.csv\")\n",
    "\n",
    "df = pd.read_csv(\n",
    "    hotel_csv,\n",
    "    # Key parameters:\n",
    "    # sep=','           → column delimiter (default: comma)\n",
    "    # header=0          → which row is the header (default: first row)\n",
    "    # dtype={'col': str} → force specific column types\n",
    "    # parse_dates=['col'] → auto-parse date columns\n",
    "    # usecols=[...]     → only load specific columns (saves memory!)\n",
    "    # nrows=1000        → only load first N rows (great for testing)\n",
    "    # na_values=['', 'NULL'] → extra strings to treat as NaN\n",
    ")\n",
    "\n",
    "# --- Quick inspection methods ---\n",
    "print(f\"Shape   : {df.shape}\")        # (rows, columns) tuple\n",
    "print(f\"Columns : {df.columns.tolist()[:10]}...\")  # column names as a list\n",
    "print(f\"Dtypes  :\\n{df.dtypes.head(10)}\")  # data type of each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Exploring the DataFrame ---\n",
    "\n",
    "# head(n) — show first n rows (default 5)\n",
    "# tail(n) — show last n rows\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# info() — summary of columns, dtypes, non-null counts, and memory usage\n",
    "# Great for spotting missing data and wrong types at a glance.\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe() — statistical summary for numeric columns\n",
    "# Shows count, mean, std, min, 25%, 50% (median), 75%, max\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 2b. read_csv with optimization — load only what you need\n",
    "# ============================================================\n",
    "# On large files, loading ALL columns wastes memory.\n",
    "# usecols selects only the columns you need.\n",
    "\n",
    "cols_needed = [\n",
    "    \"hotel\", \"is_canceled\", \"lead_time\",\n",
    "    \"arrival_date_year\", \"arrival_date_month\", \"arrival_date_day_of_month\",\n",
    "    \"stays_in_weekend_nights\", \"stays_in_week_nights\",\n",
    "    \"adults\", \"children\", \"country\",\n",
    "    \"adr\", \"reservation_status\",\n",
    "]\n",
    "\n",
    "df = pd.read_csv(\n",
    "    hotel_csv,\n",
    "    usecols=cols_needed,           # only load these columns\n",
    "    dtype={\"country\": \"category\"},  # 'category' type saves memory for low-cardinality strings\n",
    ")\n",
    "\n",
    "print(f\"Shape          : {df.shape}\")\n",
    "print(f\"Memory (full)  : {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "# memory_usage(deep=True) measures actual memory including string content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Pandas Transformation — Reshaping Values\n",
    "\n",
    "Transform columns: cast types, map values, compute new columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3a. astype — cast column data types\n",
    "# ============================================================\n",
    "# astype(new_type) converts a column to a different data type.\n",
    "# Essential for: fixing auto-detected types, optimizing memory.\n",
    "\n",
    "print(\"--- Before ---\")\n",
    "print(df[[\"is_canceled\", \"lead_time\", \"adr\"]].dtypes)\n",
    "\n",
    "# Cast is_canceled from int64 → bool (it's really a 0/1 flag)\n",
    "df[\"is_canceled\"] = df[\"is_canceled\"].astype(bool)\n",
    "\n",
    "# Cast lead_time from int64 → int32 (values are small, saves 50% memory)\n",
    "df[\"lead_time\"] = df[\"lead_time\"].astype(\"int32\")\n",
    "\n",
    "# Cast adr from float64 → float32 (sufficient precision for prices)\n",
    "df[\"adr\"] = df[\"adr\"].astype(\"float32\")\n",
    "\n",
    "print(\"\\n--- After ---\")\n",
    "print(df[[\"is_canceled\", \"lead_time\", \"adr\"]].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3b. map — transform values using a dictionary or function\n",
    "# ============================================================\n",
    "# Series.map(dict_or_func) replaces each value.\n",
    "# Like a lookup table / VLOOKUP / CASE WHEN.\n",
    "\n",
    "# Map month names to quarter numbers\n",
    "month_to_quarter = {\n",
    "    \"January\": \"Q1\", \"February\": \"Q1\", \"March\": \"Q1\",\n",
    "    \"April\": \"Q2\", \"May\": \"Q2\", \"June\": \"Q2\",\n",
    "    \"July\": \"Q3\", \"August\": \"Q3\", \"September\": \"Q3\",\n",
    "    \"October\": \"Q4\", \"November\": \"Q4\", \"December\": \"Q4\",\n",
    "}\n",
    "\n",
    "# map() replaces each value using the dictionary\n",
    "# Values not in the dict become NaN\n",
    "df[\"quarter\"] = df[\"arrival_date_month\"].map(month_to_quarter)\n",
    "\n",
    "print(df[[\"arrival_date_month\", \"quarter\"]].drop_duplicates().sort_values(\"quarter\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map() with a function — apply a function to each value\n",
    "# str.upper is a built-in string method\n",
    "df[\"hotel_upper\"] = df[\"hotel\"].map(str.upper)\n",
    "print(df[\"hotel_upper\"].unique())\n",
    "\n",
    "# Clean up the temp column\n",
    "df.drop(columns=[\"hotel_upper\"], inplace=True)\n",
    "# drop(columns=[...]) removes columns\n",
    "# inplace=True modifies the DataFrame directly instead of returning a copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3c. apply — row-wise or column-wise custom logic\n",
    "# ============================================================\n",
    "# DataFrame.apply(func, axis=1) applies a function to EACH ROW.\n",
    "#   axis=0 → apply to each column (default)\n",
    "#   axis=1 → apply to each row\n",
    "#\n",
    "# NOTE: apply(axis=1) is SLOW because it loops in Python.\n",
    "# Prefer vectorized operations (np.where, np.select) when possible.\n",
    "# Use apply only for complex logic that can't be vectorized.\n",
    "\n",
    "def compute_total_nights(row):\n",
    "    \"\"\"\n",
    "    Compute total stay length from weekend + weekday nights.\n",
    "    `row` is a Series with column names as index.\n",
    "    \"\"\"\n",
    "    return row[\"stays_in_weekend_nights\"] + row[\"stays_in_week_nights\"]\n",
    "\n",
    "\n",
    "# Slow way: apply (loops row by row in Python)\n",
    "# df[\"total_nights\"] = df.apply(compute_total_nights, axis=1)\n",
    "\n",
    "# Fast way: vectorized addition (Numpy under the hood)\n",
    "df[\"total_nights\"] = df[\"stays_in_weekend_nights\"] + df[\"stays_in_week_nights\"]\n",
    "\n",
    "# Compute total revenue (vectorized)\n",
    "df[\"total_revenue\"] = df[\"adr\"] * df[\"total_nights\"]\n",
    "\n",
    "print(df[[\"adr\", \"total_nights\", \"total_revenue\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3d. Vectorized classification with np.select (preferred over apply)\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Classify bookings by ADR tier — much faster than apply + if/elif\n",
    "conditions = [\n",
    "    df[\"adr\"] > 200,\n",
    "    df[\"adr\"] > 100,\n",
    "    df[\"adr\"] > 0,\n",
    "]\n",
    "choices = [\"Luxury\", \"Premium\", \"Budget\"]\n",
    "\n",
    "df[\"adr_tier\"] = np.select(conditions, choices, default=\"Free/Unknown\")\n",
    "\n",
    "# value_counts() returns the count of each unique value, sorted descending\n",
    "print(df[\"adr_tier\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Pandas Cleaning — Data Quality\n",
    "\n",
    "Real data is messy. These methods handle **missing values** and **duplicates**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 4a. Inspect missing data\n",
    "# ============================================================\n",
    "\n",
    "# isnull() returns a boolean DataFrame (True = missing)\n",
    "# .sum() counts True per column\n",
    "missing = df.isnull().sum()\n",
    "\n",
    "# Filter to only columns with missing values\n",
    "missing_only = missing[missing > 0].sort_values(ascending=False)\n",
    "print(\"Columns with missing values:\")\n",
    "print(missing_only)\n",
    "\n",
    "# Percentage missing\n",
    "print(f\"\\n% missing children: {df['children'].isnull().mean() * 100:.2f}%\")\n",
    "# .mean() of booleans = proportion of True values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 4b. fillna — fill missing values\n",
    "# ============================================================\n",
    "# fillna(value) replaces NaN with the specified value.\n",
    "\n",
    "# Fill missing children count with 0 (no children)\n",
    "df[\"children\"] = df[\"children\"].fillna(0)\n",
    "\n",
    "# fillna can use different strategies:\n",
    "#   fillna(0)              → fill with a constant\n",
    "#   fillna(method='ffill') → forward fill (use previous row's value)\n",
    "#   fillna(method='bfill') → backward fill (use next row's value)\n",
    "#   fillna(df['col'].mean()) → fill with column mean\n",
    "#   fillna(df['col'].median()) → fill with column median\n",
    "\n",
    "print(f\"Children NaN after fill: {df['children'].isnull().sum()}\")\n",
    "\n",
    "# Fill missing country with \"UNKNOWN\"\n",
    "df[\"country\"] = df[\"country\"].cat.add_categories(\"UNKNOWN\")  # add new category first\n",
    "# cat.add_categories() adds new valid categories to a categorical column\n",
    "# (must do this before fillna because categorical columns can only hold defined categories)\n",
    "df[\"country\"] = df[\"country\"].fillna(\"UNKNOWN\")\n",
    "print(f\"Country NaN after fill : {df['country'].isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 4c. dropna — remove rows/columns with missing data\n",
    "# ============================================================\n",
    "# dropna() removes rows (or columns) that contain NaN.\n",
    "#   axis=0     → drop ROWS (default)\n",
    "#   axis=1     → drop COLUMNS\n",
    "#   how='any'  → drop if ANY value is NaN (default)\n",
    "#   how='all'  → drop only if ALL values are NaN\n",
    "#   subset=[cols] → only check specific columns for NaN\n",
    "#   thresh=N   → keep rows with at least N non-NaN values\n",
    "\n",
    "print(f\"Rows before dropna: {len(df):,}\")\n",
    "\n",
    "# Drop rows where 'country' is missing (already filled, so just for demo)\n",
    "df_no_missing = df.dropna(subset=[\"adr\", \"country\"])\n",
    "print(f\"Rows after dropna : {len(df_no_missing):,}\")\n",
    "print(f\"Rows dropped      : {len(df) - len(df_no_missing):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 4d. drop_duplicates — remove duplicate rows\n",
    "# ============================================================\n",
    "# drop_duplicates() removes rows where ALL values are identical.\n",
    "#   subset=[cols] → only check these columns for duplicates\n",
    "#   keep='first'  → keep the first occurrence (default)\n",
    "#   keep='last'   → keep the last occurrence\n",
    "#   keep=False    → drop ALL duplicates (no rows kept)\n",
    "\n",
    "print(f\"Rows before dedup: {len(df):,}\")\n",
    "\n",
    "# Check for full-row duplicates\n",
    "n_dupes = df.duplicated().sum()  # duplicated() marks True for duplicate rows\n",
    "print(f\"Full-row duplicates: {n_dupes:,}\")\n",
    "\n",
    "# Remove duplicates\n",
    "df = df.drop_duplicates()\n",
    "print(f\"Rows after dedup : {len(df):,}\")\n",
    "\n",
    "# Dedup by specific columns (e.g., one booking per country+date)\n",
    "df_unique_country_year = df.drop_duplicates(\n",
    "    subset=[\"country\", \"arrival_date_year\"],\n",
    "    keep=\"first\"\n",
    ")\n",
    "print(f\"\\nUnique country-year combos: {len(df_unique_country_year):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Pandas Joins — Combining DataFrames\n",
    "\n",
    "- `merge()` — SQL-style joins on keys  \n",
    "- `concat()` — stack DataFrames vertically or horizontally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 5a. merge — SQL-style joins\n",
    "# ============================================================\n",
    "# pd.merge(left, right, on=..., how=...)\n",
    "#   on       : column(s) to join on (must exist in both DataFrames)\n",
    "#   left_on / right_on : use when column names differ\n",
    "#   how      : join type\n",
    "#     'inner' → only matching rows (default) — like SQL INNER JOIN\n",
    "#     'left'  → all left rows + matching right — like SQL LEFT JOIN\n",
    "#     'right' → all right rows + matching left\n",
    "#     'outer' → all rows from both sides — like SQL FULL OUTER JOIN\n",
    "\n",
    "# Create a dimension table: hotel metadata\n",
    "hotel_dim = pd.DataFrame({\n",
    "    \"hotel\": [\"Resort Hotel\", \"City Hotel\"],\n",
    "    \"city\": [\"Lagos\", \"Lisbon\"],\n",
    "    \"star_rating\": [5, 4],\n",
    "})\n",
    "\n",
    "print(\"--- Hotel Dimension ---\")\n",
    "print(hotel_dim)\n",
    "\n",
    "# LEFT JOIN: keep all bookings, enrich with hotel metadata\n",
    "df_enriched = df.merge(\n",
    "    hotel_dim,\n",
    "    on=\"hotel\",      # join column\n",
    "    how=\"left\",      # keep all rows from left (df)\n",
    ")\n",
    "\n",
    "print(f\"\\nAfter merge: {df_enriched.shape}\")\n",
    "print(df_enriched[[\"hotel\", \"city\", \"star_rating\", \"adr\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 5b. merge with different column names\n",
    "# ============================================================\n",
    "\n",
    "# Country codes dimension table (column name differs: 'code' vs 'country')\n",
    "country_dim = pd.DataFrame({\n",
    "    \"code\": [\"PRT\", \"GBR\", \"FRA\", \"ESP\", \"DEU\", \"ITA\", \"USA\", \"BRA\"],\n",
    "    \"country_name\": [\"Portugal\", \"United Kingdom\", \"France\", \"Spain\",\n",
    "                     \"Germany\", \"Italy\", \"United States\", \"Brazil\"],\n",
    "    \"region\": [\"Europe\", \"Europe\", \"Europe\", \"Europe\",\n",
    "               \"Europe\", \"Europe\", \"Americas\", \"Americas\"],\n",
    "})\n",
    "\n",
    "# Use left_on / right_on when column names don't match\n",
    "df_with_country = df.merge(\n",
    "    country_dim,\n",
    "    left_on=\"country\",    # column in left DataFrame\n",
    "    right_on=\"code\",      # column in right DataFrame\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "# Show some results (many countries won't match — that's expected)\n",
    "print(df_with_country[[\"country\", \"country_name\", \"region\"]].drop_duplicates().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 5c. concat — stack DataFrames\n",
    "# ============================================================\n",
    "# pd.concat([df1, df2, ...], axis=0/1)\n",
    "#   axis=0 → stack vertically (add rows)    — like SQL UNION ALL\n",
    "#   axis=1 → stack horizontally (add columns)\n",
    "#   ignore_index=True → reset the row index after stacking\n",
    "\n",
    "# Split data by hotel, then recombine (simulates merging daily batches)\n",
    "resort = df[df[\"hotel\"] == \"Resort Hotel\"].head(3)\n",
    "city = df[df[\"hotel\"] == \"City Hotel\"].head(3)\n",
    "\n",
    "print(f\"Resort rows: {len(resort)}, City rows: {len(city)}\")\n",
    "\n",
    "# Stack vertically (UNION ALL)\n",
    "combined = pd.concat([resort, city], axis=0, ignore_index=True)\n",
    "print(f\"Combined   : {len(combined)} rows\")\n",
    "print(combined[[\"hotel\", \"country\", \"adr\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Pandas Aggregations — Summarizing Data\n",
    "\n",
    "`groupby` + `agg` is the Pandas equivalent of SQL's `GROUP BY` with aggregate functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 6a. groupby + agg — custom aggregations\n",
    "# ============================================================\n",
    "# df.groupby(column).agg(new_col=(source_col, func))\n",
    "#\n",
    "# Named aggregation syntax:\n",
    "#   new_column_name = (\"source_column\", \"agg_function\")\n",
    "#\n",
    "# Common agg functions: 'sum', 'mean', 'median', 'min', 'max',\n",
    "#                        'count', 'nunique', 'std', 'first', 'last'\n",
    "\n",
    "hotel_summary = df.groupby(\"hotel\").agg(\n",
    "    total_bookings=(\"hotel\", \"count\"),         # COUNT(*)\n",
    "    avg_adr=(\"adr\", \"mean\"),                    # AVG(adr)\n",
    "    max_adr=(\"adr\", \"max\"),                     # MAX(adr)\n",
    "    avg_lead_time=(\"lead_time\", \"mean\"),        # AVG(lead_time)\n",
    "    unique_countries=(\"country\", \"nunique\"),    # COUNT(DISTINCT country)\n",
    "    cancel_rate=(\"is_canceled\", \"mean\"),        # AVG(is_canceled) = cancel ratio\n",
    ").reset_index()  # reset_index() moves the groupby key back to a regular column\n",
    "\n",
    "print(hotel_summary.to_string(index=False))  # to_string() for clean printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 6b. groupby with multiple keys\n",
    "# ============================================================\n",
    "# Like SQL: GROUP BY hotel, arrival_date_year\n",
    "\n",
    "yearly_summary = df.groupby([\"hotel\", \"arrival_date_year\"]).agg(\n",
    "    bookings=(\"hotel\", \"count\"),\n",
    "    avg_adr=(\"adr\", \"mean\"),\n",
    "    total_revenue=(\"total_revenue\", \"sum\"),\n",
    ").reset_index()\n",
    "\n",
    "# round() rounds all numeric columns to N decimal places\n",
    "print(yearly_summary.round(2).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 6c. Custom aggregation functions\n",
    "# ============================================================\n",
    "# You can pass your OWN function to agg().\n",
    "# The function receives a Series (all values in the group for that column).\n",
    "\n",
    "def revenue_range(series):\n",
    "    \"\"\"Compute the difference between max and min values.\"\"\"\n",
    "    return series.max() - series.min()\n",
    "\n",
    "\n",
    "custom_agg = df.groupby(\"hotel\").agg(\n",
    "    adr_range=(\"adr\", revenue_range),            # custom function\n",
    "    adr_p90=(\"adr\", lambda s: s.quantile(0.9)),  # 90th percentile via lambda\n",
    "    # quantile(0.9) returns the value below which 90% of data falls\n",
    ").reset_index()\n",
    "\n",
    "print(custom_agg.round(2).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 6d. pivot_table — spreadsheet-style aggregation\n",
    "# ============================================================\n",
    "# pivot_table() creates a cross-tabulation (rows × columns × values).\n",
    "#   index   : row labels\n",
    "#   columns : column labels\n",
    "#   values  : what to aggregate\n",
    "#   aggfunc : aggregation function (default: 'mean')\n",
    "#   fill_value : value for missing combinations\n",
    "#   margins : add row/column totals\n",
    "\n",
    "pivot = df.pivot_table(\n",
    "    index=\"hotel\",               # rows\n",
    "    columns=\"quarter\",           # columns\n",
    "    values=\"adr\",                # what to aggregate\n",
    "    aggfunc=\"mean\",              # how to aggregate\n",
    "    fill_value=0,                # fill missing combos with 0\n",
    "    margins=True,                # add \"All\" row and column (totals)\n",
    ")\n",
    "\n",
    "print(\"--- Average ADR by Hotel × Quarter ---\")\n",
    "print(pivot.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 6e. Booking count pivot with margins\n",
    "# ============================================================\n",
    "\n",
    "booking_pivot = df.pivot_table(\n",
    "    index=\"hotel\",\n",
    "    columns=\"arrival_date_year\",\n",
    "    values=\"lead_time\",          # any column works for count\n",
    "    aggfunc=\"count\",             # count rows\n",
    "    fill_value=0,\n",
    "    margins=True,\n",
    ")\n",
    "\n",
    "print(\"--- Booking Count by Hotel × Year ---\")\n",
    "print(booking_pivot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Parquet — Columnar Storage for Data Engineering\n",
    "\n",
    "**Parquet** is the standard file format for analytics and data lakes.  \n",
    "\n",
    "| Feature | CSV | Parquet |\n",
    "|---------|-----|--------|\n",
    "| Storage | Row-based (text) | Columnar (binary) |\n",
    "| Types | Everything is text | Preserves int, float, date, etc. |\n",
    "| Size | Large (no compression) | Small (built-in compression) |\n",
    "| Read speed | Must scan entire file | Can read only needed columns |\n",
    "| Append | Easy | Not easy (immutable files) |\n",
    "| Human readable | Yes | No (binary) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ============================================================\n",
    "# 7a. Writing Parquet with to_parquet\n",
    "# ============================================================\n",
    "# DataFrame.to_parquet(path, engine, compression, index)\n",
    "#   engine      : 'pyarrow' (fast, full-featured) or 'fastparquet'\n",
    "#   compression : 'snappy' (fast, default), 'gzip' (smaller), 'zstd' (balanced), None\n",
    "#   index       : whether to include the DataFrame index in the file\n",
    "\n",
    "output_dir = Path(\"data/output\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Write with default compression (snappy)\n",
    "parquet_snappy = output_dir / \"hotel_bookings_snappy.parquet\"\n",
    "df.to_parquet(\n",
    "    parquet_snappy,\n",
    "    engine=\"pyarrow\",       # pyarrow is the standard Parquet engine\n",
    "    compression=\"snappy\",   # snappy: fast compress/decompress, decent ratio\n",
    "    index=False,            # don't save the row index\n",
    ")\n",
    "\n",
    "# Write with gzip compression (smaller but slower)\n",
    "parquet_gzip = output_dir / \"hotel_bookings_gzip.parquet\"\n",
    "df.to_parquet(parquet_gzip, engine=\"pyarrow\", compression=\"gzip\", index=False)\n",
    "\n",
    "# Compare file sizes\n",
    "csv_size = Path(\"data/hotel_booking.csv\").stat().st_size / 1024**2\n",
    "snappy_size = parquet_snappy.stat().st_size / 1024**2\n",
    "gzip_size = parquet_gzip.stat().st_size / 1024**2\n",
    "\n",
    "print(f\"{'Format':<20} {'Size (MB)':>10} {'Ratio':>8}\")\n",
    "print(f\"{'-'*20} {'-'*10} {'-'*8}\")\n",
    "print(f\"{'CSV (original)':<20} {csv_size:>10.2f} {'1.00x':>8}\")\n",
    "print(f\"{'Parquet (snappy)':<20} {snappy_size:>10.2f} {csv_size/snappy_size:>7.1f}x\")\n",
    "print(f\"{'Parquet (gzip)':<20} {gzip_size:>10.2f} {csv_size/gzip_size:>7.1f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 7b. Reading Parquet with read_parquet\n",
    "# ============================================================\n",
    "# pd.read_parquet(path, engine, columns)\n",
    "#   columns : list of column names to read (HUGE performance win!)\n",
    "#             Because Parquet is columnar, it only reads the columns you need.\n",
    "\n",
    "# Read ALL columns\n",
    "df_full = pd.read_parquet(parquet_snappy, engine=\"pyarrow\")\n",
    "print(f\"Full read  : {df_full.shape} — {df_full.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Read only 3 columns (much faster on wide tables)\n",
    "df_partial = pd.read_parquet(\n",
    "    parquet_snappy,\n",
    "    engine=\"pyarrow\",\n",
    "    columns=[\"hotel\", \"country\", \"adr\"],  # only these columns are read from disk\n",
    ")\n",
    "print(f\"Partial read: {df_partial.shape} — {df_partial.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 7c. Parquet preserves data types (CSV does not)\n",
    "# ============================================================\n",
    "# When you write a CSV, everything becomes text.\n",
    "# When you read it back, Pandas must GUESS the types.\n",
    "# Parquet stores the EXACT types — no guessing, no data loss.\n",
    "\n",
    "print(\"--- Dtypes from Parquet (preserved exactly) ---\")\n",
    "print(df_full.dtypes)\n",
    "\n",
    "print(\"\\n--- Dtypes from CSV (auto-inferred, often wrong) ---\")\n",
    "df_csv = pd.read_csv(\"data/hotel_booking.csv\", usecols=[\"hotel\", \"is_canceled\", \"adr\", \"children\"])\n",
    "print(df_csv.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 7d. Inspecting Parquet metadata with PyArrow\n",
    "# ============================================================\n",
    "# PyArrow lets you read Parquet metadata WITHOUT loading the data.\n",
    "# Useful for: checking schema, row count, compression before loading.\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# pq.read_metadata() reads ONLY the file footer (fast, no data loaded)\n",
    "meta = pq.read_metadata(parquet_snappy)\n",
    "\n",
    "print(f\"Num rows       : {meta.num_rows:,}\")\n",
    "print(f\"Num columns    : {meta.num_columns}\")\n",
    "print(f\"Num row groups : {meta.num_row_groups}\")  # Parquet splits data into row groups\n",
    "print(f\"Created by     : {meta.created_by}\")\n",
    "print(f\"Format version : {meta.format_version}\")\n",
    "\n",
    "# pq.read_schema() reads the column schema (names + types)\n",
    "schema = pq.read_schema(parquet_snappy)\n",
    "print(f\"\\n--- Schema ---\")\n",
    "for i in range(len(schema)):\n",
    "    field = schema.field(i)  # field(i) returns the i-th column definition\n",
    "    print(f\"  {field.name:<30} {str(field.type):<15}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 7e. Practical — CSV to Parquet conversion pipeline\n",
    "# ============================================================\n",
    "# A common DE task: convert raw CSV → optimized Parquet for downstream use.\n",
    "\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "csv_source = Path(\"data/hotel_booking.csv\")\n",
    "parquet_dest = Path(\"data/output/hotel_bookings_clean.parquet\")\n",
    "\n",
    "# --- Step 1: Read CSV with optimized types ---\n",
    "start = time.perf_counter()\n",
    "\n",
    "df_raw = pd.read_csv(\n",
    "    csv_source,\n",
    "    dtype={\n",
    "        \"hotel\": \"category\",               # low-cardinality → category\n",
    "        \"country\": \"category\",\n",
    "        \"market_segment\": \"category\",\n",
    "        \"reserved_room_type\": \"category\",\n",
    "        \"is_canceled\": \"int8\",              # 0/1 flag → int8 (1 byte vs 8)\n",
    "        \"lead_time\": \"int32\",               # small ints → int32 (4 bytes vs 8)\n",
    "        \"adr\": \"float32\",                   # prices → float32 (4 bytes vs 8)\n",
    "    },\n",
    ")\n",
    "\n",
    "csv_time = time.perf_counter() - start\n",
    "print(f\"CSV read     : {csv_time:.3f}s, {df_raw.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "\n",
    "# --- Step 2: Write Parquet ---\n",
    "start = time.perf_counter()\n",
    "df_raw.to_parquet(parquet_dest, engine=\"pyarrow\", compression=\"snappy\", index=False)\n",
    "write_time = time.perf_counter() - start\n",
    "print(f\"Parquet write: {write_time:.3f}s, {parquet_dest.stat().st_size / 1024**2:.1f} MB on disk\")\n",
    "\n",
    "# --- Step 3: Read Parquet back ---\n",
    "start = time.perf_counter()\n",
    "df_pq = pd.read_parquet(parquet_dest, engine=\"pyarrow\")\n",
    "pq_time = time.perf_counter() - start\n",
    "print(f\"Parquet read : {pq_time:.3f}s, {df_pq.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "\n",
    "print(f\"\\nRead speedup : {csv_time / pq_time:.1f}x faster than CSV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Key Takeaways\n",
    "\n",
    "| Concept | Key Functions | When to Use |\n",
    "|---------|--------------|-------------|\n",
    "| **Numpy** | `np.where`, `np.select`, `np.unique`, `np.isnan`, `nanmean` | Vectorized math, conditional logic, missing data |\n",
    "| **read_csv** | `usecols`, `dtype`, `nrows`, `parse_dates` | Load CSVs with memory optimization |\n",
    "| **astype / map** | `astype('category')`, `map(dict)` | Cast types, map values |\n",
    "| **apply** | `apply(func, axis=1)` | Complex row logic (prefer vectorized!) |\n",
    "| **fillna / dropna** | `fillna(0)`, `dropna(subset=[...])` | Handle missing data |\n",
    "| **drop_duplicates** | `drop_duplicates(subset=[...])` | Remove duplicate rows |\n",
    "| **merge** | `merge(on=..., how='left')` | SQL-style joins between DataFrames |\n",
    "| **concat** | `pd.concat([df1, df2])` | Stack DataFrames (UNION ALL) |\n",
    "| **groupby + agg** | `.groupby().agg(new=(col, func))` | GROUP BY with named aggregations |\n",
    "| **pivot_table** | `pivot_table(index, columns, values)` | Cross-tabulation summaries |\n",
    "| **Parquet** | `to_parquet`, `read_parquet`, `pq.read_schema` | Fast, typed, compressed columnar storage |\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Continue with **`10_sql_analytics.ipynb`** — DuckDB: run SQL directly on CSVs, Parquet files, and DataFrames."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
